[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GEDI / ICESAT-2 Workshop",
    "section": "",
    "text": "Welcome to the 2023 GEDI / ICESAT-2 Application Workshop hosted by the Space and Sustainability Colloquium with support from NASA Openscapes.\nThe workshop will take place in-person on November 16 2023 from 3:00pm-6:00pm CT (UTC -6).\nGET STARTED:\n\nDeploy Jupyter Lab instance in 2i2c\n\nWorkshop Schedule\nAccess notebooks without cloning"
  },
  {
    "objectID": "index.html#welcome-bienvenidos",
    "href": "index.html#welcome-bienvenidos",
    "title": "GEDI / ICESAT-2 Workshop",
    "section": "",
    "text": "Welcome to the 2023 GEDI / ICESAT-2 Application Workshop hosted by the Space and Sustainability Colloquium with support from NASA Openscapes.\nThe workshop will take place in-person on November 16 2023 from 3:00pm-6:00pm CT (UTC -6).\nGET STARTED:\n\nDeploy Jupyter Lab instance in 2i2c\n\nWorkshop Schedule\nAccess notebooks without cloning"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "GEDI / ICESAT-2 Workshop",
    "section": "About",
    "text": "About\n\nWorkshop Description\nThe workshop will demonstrate how to find, access, and work with GEDI and Icesat-2 data from the Earthdata Cloud. Participants will learn how to search for and download data from NASA‚Äôs Earthdata Search Client, a graphical user interface (GUI) for search, discovery, and download application for also EOSDIS data assets. Participants will learn how to perform in-could data search, access, and processing routines where no data download is required, and data analysis can take place next to the data in the cloud."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "GEDI / ICESAT-2 Workshop",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n2023 GEDI/ICESAT-2 Workshop is hosted by the Space and Sustainability Colloquium with support from the NASA Openscapes Project, with cloud computing infrastructure by 2i2c."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html",
    "title": "Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html#summary",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html#summary",
    "title": "Authentication for NASA Earthdata",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin &lt;USERNAME&gt;\npassword &lt;PASSWORD&gt;\n&lt;USERNAME&gt; and &lt;PASSWORD&gt; would be replaced by your actual Earthdata Login username and password respectively."
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Authentication.html#import-required-packages",
    "href": "how-tos/authentication/NASA_Earthdata_Authentication.html#import-required-packages",
    "title": "Authentication for NASA Earthdata",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} &gt;&gt; {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} &gt;&gt; {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'&gt;&gt; {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\nSee if the file was created\nIf the file was created, we‚Äôll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path‚Ä¶, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/\n\ntotal 128\ndrwxr-xr-x 25 jovyan jovyan  6144 Nov 11 18:59  .\ndrwxr-xr-x  1 root   root      20 Mar 11  2022  ..\n-rw-------  1 jovyan jovyan 12347 Nov 11 20:21  .bash_history\ndrwxr-xr-x  8 jovyan jovyan  6144 Nov  8 20:16  .cache\ndrwxrwsr-x  2 jovyan jovyan  6144 Apr 12  2022  .conda\ndrwxr-xr-x  4 jovyan jovyan  6144 Feb 28  2022  .config\ndrwx------  2 jovyan jovyan  6144 Nov 11 17:36  .git-credential-cache\n-rw-r--r--  1 jovyan jovyan    56 Nov  4 15:14  .gitconfig\ndrwxr-xr-x  3 jovyan jovyan  6144 Apr 13  2022  .hidden_dir\ndrwxr-xr-x  2 jovyan jovyan  6144 Nov 11 02:45  .ipynb_checkpoints\ndrwxr-xr-x  5 jovyan jovyan  6144 Jul 20  2021  .ipython\ndrwxr-xr-x  3 jovyan jovyan  6144 Jul 20  2021  .jupyter\n-rw-r--r--  1 jovyan jovyan     0 Jul 20  2021  .jupyter-server-log.txt\ndrwxr-xr-x  3 jovyan jovyan  6144 Jul 20  2021  .local\n-rw-------  1 jovyan jovyan    73 Nov 11 18:24  .netrc\ndrwx------  2 jovyan jovyan  6144 Aug  4 15:42  .ssh\ndrwxr-xr-x 12 jovyan jovyan  6144 Nov 11 05:57  2022-ECOSTRESS-Cloud-Workshop\ndrwxr-xr-x 13 jovyan jovyan  6144 Nov 14 05:11  2022-Fall-ECOSTRESS-Cloud-Workshop\ndrwxr-xr-x 15 jovyan jovyan  6144 Nov  9 19:18  2022-Fall-ECOSTRESS-Cloud-Workshop_MJ\ndrwxr-xr-x 13 jovyan jovyan  6144 Nov 11 16:43  2022-Fall-ECOSTRESS-Cloud-Workshop_mmm\ndrwxr-xr-x  5 jovyan jovyan  6144 Nov 10 14:37 'Untitled Folder'\ndrwxr-xr-x  5 jovyan jovyan  6144 Aug 29 15:17  appeears-cloud-optimized-format-prototype\n-rw-r--r--  1 jovyan jovyan   131 Nov 11 18:59  cookies.txt\ndrwxr-xr-x 15 jovyan jovyan  6144 Mar 10  2022  earthdata-cloud-cookbook\ndrwxr-xr-x  6 jovyan jovyan  6144 Apr 28  2022  lpdaac_cloud_data_access\ndrwxr-xr-x  6 jovyan jovyan  6144 Oct 20  2021  lpdaac_cloud_data_access1\ndrwxr-xr-x  4 jovyan jovyan  6144 Jul 20  2021  lpdaac_hls_tutorial\ndrwxr-xr-x  4 jovyan jovyan  6144 Sep 19 21:29  mentors-2022\ndrwxr-xr-x 27 jovyan jovyan  6144 Oct 28 01:20  shared\ndrwxr-xr-x 27 jovyan jovyan  6144 Oct 28 01:20  shared-readwrite"
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g.¬†zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g.¬†zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#what-is-xarray",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#what-is-xarray",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You‚Äôll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we‚Äôll start by importing xarray. We‚Äôll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\nI‚Äôm going to use one of xarray‚Äôs tutorial datasets. In this case, air temperature from the NCEP reanalysis. I‚Äôll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\n\nds\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds"
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It‚Äôs also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I‚Äôll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as ‚Äúnearest‚Äù.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')"
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#analysis",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#analysis",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let‚Äôs try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let‚Äôs calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()\nds_clim"
  },
  {
    "objectID": "how-tos/data-access/Intro_xarray_hvplot.html#plot-results",
    "href": "how-tos/data-access/Intro_xarray_hvplot.html#plot-results",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "Plot results",
    "text": "Plot results\nFinally, let‚Äôs plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()"
  },
  {
    "objectID": "tutorials/data-access/earthdata-search.html",
    "href": "tutorials/data-access/earthdata-search.html",
    "title": "Earthdata Search",
    "section": "",
    "text": "Earthdata Search\nThis tutorial guides you through how to use Earthdata Search for NASA Earth observations search and discovery, and how to connect the search output (e.g.¬†download or access links) to a programmatic workflow (locally or from within the cloud).\n\nStep 1. Go to Earthdata Search and Login\nGo to Earthdata Search https://search.earthdata.nasa.gov and use your Earthdata login credentials to log in. If you do not have an Earthdata account, please see the Workshop Prerequisites for guidance.\n\n\nStep 2. Search for dataset of interest\nUse the search box in the upper left to type key words. In this example we are interested in the GEDI or ICESAT-2 which is managed by the LP DAAC and made available from the NASA Earthdata Cloud archive hosted in AWS cloud.\nType GEDI in the search bar Click on the ‚ÄúAvailable from AWS Cloud‚Äù filter option on the left.\nWe can click on the (i) icon for the dataset to read more details, including the dataset shortname (helpful for programmatic workflows) just below the dataset name; here ECO_L2T_LSTE.\n\n\nStep 3. Explore the dataset details, including Cloud Access information\nOnce we clicked the (i), scrolling down the info page for the dataset we will see Cloud Access information, such as:\n\nwhether the dataset is available in the cloud\n\nthe cloud Region (all NASA Earthdata Cloud data is/will be in us-west-2 region)\n\nthe S3 storage bucket and object prefix where this data is located\n\nlink that generates AWS S3 Credentials for in-cloud data access (we will cover this in the Direct Data Access Tutorials)\n\nlink to documentation describing the In-region Direct S3 Access to Buckets. Note: these will be unique depending on the DAAC where the data is archived. (We will show examples of direct in-region access in Tutorial 3.)\n\n\n\n\nFigure caption: Cloud access info in EDS\n\n\n\n\n\nFigure caption: Documentation describing the In-region Direct S3 Access to Buckets\n\n\nNote: Clicking on ‚ÄúFor Developers‚Äù to exapnd will provide programmatic endpoints such as those for the CMR API, and more.\nFor now, let‚Äôs say we are intersted in getting download link(s) or access link(s) for specific data files (granules) within this collection.\nAt the top of the dataset info section, click on Search Results, which will take us back to the list of datasets matching our search parameters. Clicking on the dataset (ECOSTRESS ECO_L2T_LSTE) we now see a list of files (granules) that are part of the dataset (collection).\n\n\nStep 4a. Download or data access for a single granule\nTo download files for a granule click the download arrow on the card (or list row)\n\n\n\nFigure caption: Download granules\n\n\nYou can also get the S3 information (e.g., AWS region, bucket, temperary credentials for S3 access, and file names) by selecting the AWS S3 Access tab.\n\n\n\nFigure caption: S3 access for granules\n\n\n\nStep 4b. Download or data access for multiple granule\nTo download multiple granules, click on the green + symbol to add files to our project. Click on the green button towards the bottom that says ‚ÄúDownload‚Äù. This will take us to another page with options to customize our download or access link(s).\n\n\n\nFigure caption: Select granules and click download\n\n\nOn the next page click the Direct Download option and click the green Download Data on the bottom left side of the page.\n\n\n\nFigure caption: Direct download multiple granules\n\n\nWe‚Äôre now taked to the final page for instructions to download and links for data access in the cloud. You should see three tabs: Download Files, AWS S3 Access, Download Script:\n\n\n\nFigure caption: Download to local\n\n\n\n\n\nFigure caption: Direct S3 access\n\n\nThe Download Files tab provides the https:// links for downloading the files locally\nThe AWS S3 Access tab provides the S3:// links, which is what we would use to access the data directly in-region (us-west-2) within the AWS cloud."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Overview",
    "section": "",
    "text": "These tutorials are a combination of narrative, links, code, and outputs. They have been developed for live demos during the Workshop, and are available for self-paced learning.\nHands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Please pass along your Github Username to get access.\nTutorials are markdown (.md) and Jupyter (.ipynb) notebooks, and are available on GitHub"
  },
  {
    "objectID": "tutorials/setup.html",
    "href": "tutorials/setup.html",
    "title": "Setup for tutorials",
    "section": "",
    "text": "This tutorial will help you set up your JupyterHub (or Hub) with tutorials and other materials from our Workshop folder."
  },
  {
    "objectID": "tutorials/setup.html#step-1.-login-to-the-hub",
    "href": "tutorials/setup.html#step-1.-login-to-the-hub",
    "title": "Setup for tutorials",
    "section": "Step 1. Login to the Hub",
    "text": "Step 1. Login to the Hub\nPlease go to Jupyter Hub and Log in with your GitHub Account, and select ‚ÄúSmall‚Äù.\nAlternatively, you can also click this badge to launch the Hub:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: It takes a few minutes for the Hub to load. Please be patient!\n\nWhile the Hub loads, we‚Äôll:\n\nDiscuss cloud environments\n\nSee how my Desktop is setup\n\nDiscuss python and conda environments\n\nThen, when the Hub is loaded, we‚Äôll get oriented in the Hub."
  },
  {
    "objectID": "tutorials/setup.html#discussion-cloud-environment",
    "href": "tutorials/setup.html#discussion-cloud-environment",
    "title": "Setup for tutorials",
    "section": "Discussion: Cloud environment",
    "text": "Discussion: Cloud environment\nA brief overview. See NASA Openscapes Cloud Environment in the 2021-Cloud-Hackathon book for more detail.\n\nCloud infrastructure\n\nCloud: AWS us-west-2\n\nData: AWS S3 (cloud) and NASA DAAC data centers (on-prem).\n\nCloud compute environment: 2i2c Jupyterhub deployment\n\nIDE: JupyterLab"
  },
  {
    "objectID": "tutorials/setup.html#discussion-my-desktop-setup",
    "href": "tutorials/setup.html#discussion-my-desktop-setup",
    "title": "Setup for tutorials",
    "section": "Discussion: My desktop setup",
    "text": "Discussion: My desktop setup\nI‚Äôll screenshare to show and/or talk through how I have oriented the following software we‚Äôre using:\n\nWorkshop Book\nSlack"
  },
  {
    "objectID": "tutorials/setup.html#discussion-python-and-conda-environments",
    "href": "tutorials/setup.html#discussion-python-and-conda-environments",
    "title": "Setup for tutorials",
    "section": "Discussion: Python and Conda environments",
    "text": "Discussion: Python and Conda environments\nWhy Python?\n\n\n\nPython Data Stack. Source: Jake VanderPlas, ‚ÄúThe State of the Stack,‚Äù SciPy Keynote (SciPy 2015).\n\n\nDefault Python Environment:\nWe‚Äôve set up the Python environment with conda.\n\n\n\n\n\n\nConda environment\n\n\n\n\n\nname: openscapes\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - pangeo-notebook\n  - awscli~=1.20\n  - boto3~=1.19\n  - gdal~=3.3\n  - rioxarray~=0.8\n  - xarray~=0.19\n  - h5netcdf~=0.11\n  - netcdf4~=1.5\n  - h5py~=2.10\n  - geoviews~=1.9\n  - matplotlib-base~=3.4\n  - hvplot~=0.7\n  - pyproj~=3.2\n  - bqplot~=0.12\n  - geopandas~=0.10\n  - zarr~=2.10\n  - cartopy~=0.20\n  - shapely==1.7.1\n  - pyresample~=1.22\n  - joblib~=1.1\n  - pystac-client~=0.3\n  - s3fs~=2021.7\n  - ipyleaflet~=0.14\n  - sidecar~=0.5\n  - jupyterlab-geojson~=3.1\n  - jupyterlab-git\n  - jupyter-resource-usage\n  - ipympl~=0.6\n  - conda-lock~=0.12\n  - pooch~=1.5\n  - pip\n  - pip:\n    - tqdm\n    - harmony-py\n    - earthdata\n    - zarr-eosdis-store\n\n\n\n\nBash terminal and installed software\nLibraries that are available from the terminal\n\ngdal 3.3 commands ( gdalinfo, gdaltransform‚Ä¶)\nhdf5 commands ( h5dump, h5ls..)\nnetcdf4 commands (ncdump, ncinfo ‚Ä¶)\njq (parsing json files or streams from curl)\ncurl (fetch resources from the web)\nawscli (AWS API client, to interact with AWS cloud services)\nvim (editor)\ntree ( directory tree)\nmore ‚Ä¶\n\n\n\nUpdating the environment\nScientific Python is a vast space and we only included libraries that are needed in our tutorials. Our default environment can be updated to include any Python library that‚Äôs available on pip or conda.\nThe project used to create our default environment is called corn (as it can include many Python kernels).\nIf we want to update a library or install a whole new environment we need to open an issue on this repository.\n\n\ncorn üåΩ"
  },
  {
    "objectID": "tutorials/setup.html#step-2.-jupyterhub-orientation",
    "href": "tutorials/setup.html#step-2.-jupyterhub-orientation",
    "title": "Setup for tutorials",
    "section": "Step 2. JupyterHub orientation",
    "text": "Step 2. JupyterHub orientation\nNow that the Hub is loaded, let‚Äôs get oriented.\n\n\n\n\n\n\nFirst impressions\n\nLauncher & the big blue button\n‚Äúhome directory‚Äù"
  },
  {
    "objectID": "tutorials/setup.html#step-3.-navigate-to-the-workshop-folder",
    "href": "tutorials/setup.html#step-3.-navigate-to-the-workshop-folder",
    "title": "Setup for tutorials",
    "section": "Step 3. Navigate to the Workshop folder",
    "text": "Step 3. Navigate to the Workshop folder\nThe workshop folder 2022-ECOSTRESS-Cloud-Workshop is in the shared folder on JupyterHub."
  },
  {
    "objectID": "tutorials/setup.html#jupyter-notebooks",
    "href": "tutorials/setup.html#jupyter-notebooks",
    "title": "Setup for tutorials",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nLet‚Äôs get oriented to Jupyter notebooks, which we‚Äôll use in all the tutorials."
  },
  {
    "objectID": "tutorials/setup.html#how-do-i-end-my-session",
    "href": "tutorials/setup.html#how-do-i-end-my-session",
    "title": "Setup for tutorials",
    "section": "How do I end my session?",
    "text": "How do I end my session?\n(Also see How do I end my Openscapes session? Will I lose all of my work?) When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save money and is a good habit to be in. When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\nStopping the server happens automatically when you log out, so navigate to ‚ÄúFile -&gt; Log Out‚Äù and click ‚ÄúLog Out‚Äù!\n\n\n\nhub-control-panel-button (credit: UW Hackweek)\n\n\n!!! NOTE ‚Äúlogging out‚Äù - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day."
  },
  {
    "objectID": "tutorials/cloud/index.html",
    "href": "tutorials/cloud/index.html",
    "title": "CryoCloud / Openscapes",
    "section": "",
    "text": "CryoCloud / Openscapes\nPlaceholder"
  },
  {
    "objectID": "tutorials/science/Intro_xarray_hvplot.html",
    "href": "tutorials/science/Intro_xarray_hvplot.html",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g.¬†zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/science/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "href": "tutorials/science/Intro_xarray_hvplot.html#why-do-we-need-xarray",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g.¬†zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors."
  },
  {
    "objectID": "tutorials/science/Intro_xarray_hvplot.html#what-is-xarray",
    "href": "tutorials/science/Intro_xarray_hvplot.html#what-is-xarray",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "What is xarray",
    "text": "What is xarray\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You‚Äôll be more attractive, more interesting, and better equiped to take on lifes challenges."
  },
  {
    "objectID": "tutorials/science/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "href": "tutorials/science/Intro_xarray_hvplot.html#what-you-will-learn-from-this-tutorial",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "What you will learn from this tutorial",
    "text": "What you will learn from this tutorial\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we‚Äôll start by importing xarray. We‚Äôll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\nI‚Äôm going to use one of xarray‚Äôs tutorial datasets. In this case, air temperature from the NCEP reanalysis. I‚Äôll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\n\nds\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds"
  },
  {
    "objectID": "tutorials/science/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "href": "tutorials/science/Intro_xarray_hvplot.html#subsetting-and-indexing",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "Subsetting and Indexing",
    "text": "Subsetting and Indexing\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It‚Äôs also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I‚Äôll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as ‚Äúnearest‚Äù.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')"
  },
  {
    "objectID": "tutorials/science/Intro_xarray_hvplot.html#analysis",
    "href": "tutorials/science/Intro_xarray_hvplot.html#analysis",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "Analysis",
    "text": "Analysis\nAs a simple example, let‚Äôs try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let‚Äôs calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()\nds_clim"
  },
  {
    "objectID": "tutorials/science/Intro_xarray_hvplot.html#plot-results",
    "href": "tutorials/science/Intro_xarray_hvplot.html#plot-results",
    "title": "Introduction to xarray‚Ä¶ and hvplot",
    "section": "Plot results",
    "text": "Plot results\nFinally, let‚Äôs plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()"
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html",
    "title": "Demo JupyterHub",
    "section": "",
    "text": "Author: Tasha Snow\nfidtlihvarkv Learning Objectives - **Learn how to access and use the Openscapes JupyterHub**  - **Open the JupyterHub and clone the Openscapes Espacio and Sostenibilidad Colloquium repository**"
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html#access-the-cryocloud-powerpoint-whenever-you-need-to-reference-it",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html#access-the-cryocloud-powerpoint-whenever-you-need-to-reference-it",
    "title": "Demo JupyterHub",
    "section": "Access the CryoCloud powerpoint whenever you need to reference it",
    "text": "Access the CryoCloud powerpoint whenever you need to reference it\nOpen the powerpoint by directly clicking on the hyperlink above or to open it in the Openscapes Linux Desktop web browser as follows: - Copy this hyperlink: https://bit.ly/4785hMv - Click on the plus (+) sign in the File Browser to the left to open a Launcher window - Under Notebooks, click on Desktop to access the Linux Desktop. This will open a new tab. - Click on the Web Browser tool (globe) at the bottom of the screen - Paste the url into the search bar"
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html#open-cryocloud",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html#open-cryocloud",
    "title": "Demo JupyterHub",
    "section": "Open CryoCloud",
    "text": "Open CryoCloud\n\nScroll through the server sizes. Stick with the 3.7Gb server (the default).\n\n```fidtlihvarkv Tip Be realistic about the max memory you will need. The amount you select, you are guaranteed, but if you use more you risk crashing your server for you and anyone else who is sharing with you. If you crash the server, it just requires logging out and reopening it, but it could be annoying for everyone.\nCheck your memory usage at the bottom in the middle of the screen.\n\n2) Choose the Python programming language.\n\n3) Sit back and learn about each of the tools!\n    - JupyterHub options and viewing setup\n    - Github\n    - Virtual Linux desktop\n    - SyncThing\n    - Viewing and editing of different files\n\nNow after the demo...\n\n## Task: Clone the Espacio and Sostenibilidad Colloquium jupyterbook\n\nWe will import the [NASA Openscapes Espacio and Sostenibilidad Colloquium Github repository](https://github.com/NASA-Openscapes/2023-ssc.git).\n\nTo do this: \n1. Select the plus (`+`) sign above the `File Browser` to the left, which will bring up a `Launcher` window. \n\n2. Click the `terminal` button under Other to open it. This is your command line like you would have on any computer. \n\nBefore cloning the repo, you have the option to switch to another file folder using the _change directory_ terminal command: `cd folder` if you do not want the Hackweek repo in your current directory (you can check which directory you are currently in using _print working directory_ command: `pwd`).\ncd yourfoldername\n\n3. Now clone the hackweek code into your current directory: \ngit clone https://github.com/NASA-Openscapes/2023-ssc.git\n\n4. You will see the folder pop into your `File Browser` on the left if you have the current directory open. Click on the folder to navigate through the files. \n\n5. To open this tutorial, click on the `book` subdirectory &gt; `tutorials` &gt; `jupyterhub_demo` &gt; and double click on `jupyterhub_demo`. This should open up this tutorial in case you want to review it in the future. \n\n## Shutting down your JupyterHub\n\n```{admonition} TIP\n**Best Practice: Shut down the Openscapes server when you are done to save us money.**\n\n**If you only close your tab or click log out, your server will continue running for 90 minutes.**\nWhenever you are done, it is best to shut down your server when you sign out to save money. Time on the JupyterHub costs money and there are systems in place to make sure your server doesn‚Äôt run indefinitely if you forget about it. After 90 minutes of no use, it will shut down. We prefer you shut down the server when so we save that 90 minutes of computing cost. To do so:\n\nIn upper left, click on File &gt; Hub Control Panel, which will open another tab\nClick the Stop Server button. Once this button disappears after you clicked it, your server is off.\nClick Log Out in the top right of your screen and you will be logged out, or you can start a new server\nYou can now close this tab and the other tab where you were just working"
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html#summary",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html#summary",
    "title": "Demo JupyterHub",
    "section": "Summary",
    "text": "Summary\nüéâ Congratulations! You‚Äôve completed this tutorial and have seen how we can access and use the Openscapes JupyterHub."
  },
  {
    "objectID": "tutorials/jupyterhub_demo/jupyterhub_demo.html#references",
    "href": "tutorials/jupyterhub_demo/jupyterhub_demo.html#references",
    "title": "Demo JupyterHub",
    "section": "References",
    "text": "References\nTo learn more about CryoCloud, gain code for NASA data access, and find other Cryosphere tutorials check out this other documentation:\n\nCryoCloud JupyterBook"
  },
  {
    "objectID": "tutorials/prerequisites.html",
    "href": "tutorials/prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "To follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don‚Äôt already have one) at https://github.com. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don‚Äôt already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nNetrc file\n\nCreate a netrc file using your Earthdata login credentials using the instruction.\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.\n\nSlack\n\nThe event Slack channel"
  },
  {
    "objectID": "tutorials/prerequisites.html#prerequisites",
    "href": "tutorials/prerequisites.html#prerequisites",
    "title": "Prerequisites",
    "section": "",
    "text": "To follow along hands-on during the Workshop, please do the following (20 minutes). All software or accounts are free.\n\nGitHub username\n\nCreate a GitHub account (if you don‚Äôt already have one) at https://github.com. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don‚Äôt already have one) at https://urs.earthdata.nasa.gov\nRemember your username and password; you will need to download or access cloud data during the workshop and beyond.\n\nNetrc file\n\nCreate a netrc file using your Earthdata login credentials using the instruction.\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.\n\nSlack\n\nThe event Slack channel"
  },
  {
    "objectID": "tutorials/further-resources.html",
    "href": "tutorials/further-resources.html",
    "title": "Additional resources",
    "section": "",
    "text": "One stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "tutorials/further-resources.html#a-growing-list-of-resources",
    "href": "tutorials/further-resources.html#a-growing-list-of-resources",
    "title": "Additional resources",
    "section": "",
    "text": "One stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "tutorials/further-resources.html#additional-tutorials",
    "href": "tutorials/further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics"
  },
  {
    "objectID": "tutorials/GEDI_data_SSC23.html#helpful-links",
    "href": "tutorials/GEDI_data_SSC23.html#helpful-links",
    "title": "How to work with GEDI Level 2B V002 Data",
    "section": "Helpful Links",
    "text": "Helpful Links\n\nLP DAAC Website\nLP DAAC GitHub\nGEDI Data Resources GitHub\nGEDI Data Product Pages\nUniversity of Maryland GEDI - Learn more about the GEDI Mission\n\nOpenAltimetry - Learn about GEDI coverage\n\nNASA Earthdata Search"
  },
  {
    "objectID": "tutorials/GEDI_data_SSC23.html#contact-info",
    "href": "tutorials/GEDI_data_SSC23.html#contact-info",
    "title": "How to work with GEDI Level 2B V002 Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¬π\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-15-2023\n¬πWork performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "tutorials/cloud/cloud-paradigm.html",
    "href": "tutorials/cloud/cloud-paradigm.html",
    "title": "NASA and the Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "tutorials/schedule.html",
    "href": "tutorials/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The SSC GEDI/ICESAT-2 Workshop will take place on November 15th.\nNote, hands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Please pass along your Github Username to get access."
  },
  {
    "objectID": "tutorials/schedule.html#workshop-schedule",
    "href": "tutorials/schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\n\n\nHour\nEvent\nInstructor\n\n\n\n\n3:00 pm CST/MX\nWelcome and Overview of Tutorial\n\n\n\n3:10 pm\nCryoCloud - A Shared Cloud Platform for NASA\nTasha Snow (Colorado School of Mines)\n\n\n3:25 pm\nHub task - clone repo\n\n\n\n3:35 pm\nBreak (Q&A)\n\n\n\n3:40 pm\nICESat-2 Data Access, NASA DAAC @ NSIDC\nLuis Alberto Lopez Espinosa  (NSIDC, University of Colorado)\n\n\n4:00 pm\nBreak (Q&A)\n\n\n\n4:10 pm\nThe icepyx Software Library and Community\nRachel Wegener (University of Maryland)\n\n\n4:30 pm\nBreak\n\n\n\n4:40 pm\nAccess and Discovery of GEDI Data via the DAAC at the ORNL\nRupesh Shrestha (Oakridge National Laboratory, ORNL-DAAC)\n\n\n5:00 pm\nBreak (Q&A)\n\n\n\n5:10 pm\nGEDI Data User Resources at the LP DAAC\nMahsa Jami (NASA LP-DAAC)\n\n\n5:30 pm\nBreak (Q&A)\n\n\n\n5:40 pm\nEjemplos de aplicaciones de uso de √©xito GEDI y posibilidades(Mapas, Reforestacion, Especie protegida)\nAdrian Pascual\n\n\n6:00 pm CST/MX\nEnd of Tutorial\n\n\n\n\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nYou will continue to have access to the 2i2c JupyterHub in AWS for a week following the SSC GEDI/ICESAT-2 Cloud Workshop. You may use that time to continue work and all learn more about migrating data accass routines and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project."
  },
  {
    "objectID": "tutorials/schedule.html#getting-help-during-the-workshop",
    "href": "tutorials/schedule.html#getting-help-during-the-workshop",
    "title": "Schedule",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nPlease use the webex chat platform to ask any questions you have during the workshop."
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "",
    "text": "The Global Ecosystem Dynamics Investigation (GEDI) mission aims to characterize ecosystem structure and dynamics to enable radically improved quantification and understanding of the Earth‚Äôs carbon cycle and biodiversity. The GEDI instrument produces high resolution laser ranging observations of the 3-dimensional structure of the Earth. GEDI is attached to the International Space Station and collects data globally between 51.6 N and 51.6 S latitudes at the highest resolution and densest sampling of any light detection and ranging (lidar) instrument in orbit to date. The Land Processes Distributed Active Archive Center (LP DAAC) distributes the GEDI Level 1 and Level 2 Version 1 and Version 2 products. The L1B and L2 GEDI products are archived and distributed in the HDF-EOS5 file format."
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#use-case-example",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#use-case-example",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "Use Case Example:",
    "text": "Use Case Example:\nThis tutorial was developed using an example use case for a project being completed by the National Park Service. The goal of the project is to use GEDI L2B Version 2 data to observe tree canopy height, cover, and profile over Redwood National Park in northern California.\nThis tutorial will show how to use Python to open GEDI L2B Version 2 files, visualize the sub-orbit of GEDI points (shots), subset to a region of interest, visualize GEDI canopy height and vertical profile metrics, and export subsets of GEDI science dataset (SDS) layers as GeoJSON files that can be loaded into GIS and/or Remote Sensing software programs.\n\nRedwood National Park GeoJSON\n\nContains the administrative boundary for Redwood National Park, available from: Administrative Boundaries of National Park System Units 12/31/2017 - National Geospatial Data Asset (NGDA) NPS National Parks Dataset\n\n\n\nData Used in the Example:\n\nGEDI L2B Canopy Cover and Vertical Profile Metrics Data Global Footprint Level - GEDI02_B.002\n\nThe purpose of the L2B dataset is to extract biophysical metrics from each GEDI waveform. These metrics are based on the directional gap probability profile derived from the L1B waveform and include canopy cover, Plant Area Index (PAI), Plant Area Volume Density (PAVD) and Foliage Height Diversity (FHD).\n\nScience Dataset (SDS) layers:\n\n/geolocation/digital_elevation_model\n/geolocation/elev_lowestmode\n\n/geolocation/elev_highestreturn\n\n/geolocation/lat_lowestmode\n\n/geolocation/lon_lowestmode\n\n/rh100\n\n/l2b_quality_flag\n\n/degrade_flag\n\n/sensitivity\n\n/pai\n\n/pavd_z\n\n/geolocation/shot_number\n\n/dz\n\n/selected_l2a_algorithm"
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#source-code-used-to-generate-this-tutorial",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#source-code-used-to-generate-this-tutorial",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "Source Code used to Generate this Tutorial:",
    "text": "Source Code used to Generate this Tutorial:\nThe repository containing all of the required files is located at: https://github.com/nasa/GEDI-Data-Resources\n\nNOTE: This tutorial was developed for GEDI L2B Version 2 HDF-EOS5 files and should only be used for that product."
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#import-packages",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#import-packages",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "1.1 Import Packages ",
    "text": "1.1 Import Packages \n\nImport the required packages and set the input/working directory to run this Jupyter Notebook locally.\n\nimport os\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport geopandas as gp\nfrom shapely.geometry import Point\nimport geoviews as gv\nfrom geoviews import opts, tile_sources as gvts\nimport holoviews as hv\ngv.extension('bokeh', 'matplotlib')\nimport shapely\nimport earthaccess\nimport warnings\nfrom shapely.errors import ShapelyDeprecationWarning\nwarnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)"
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#set-up-the-working-environment-and-retrieve-files",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#set-up-the-working-environment-and-retrieve-files",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "1.2 Set Up the Working Environment and Retrieve Files",
    "text": "1.2 Set Up the Working Environment and Retrieve Files\n\nThe input directory is defined as the current working directory.\n\ninDir = os.getcwd()   # Set input directory to the current working directory\nos.chdir(inDir)\ndata_dir = inDir.rsplit('2023-ssc')[0] + 'shared/2023SSC/'\ndata_dir\n\n'/home/jovyan/shared/2023SSC/'"
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#authentication",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#authentication",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "1.3 Authentication",
    "text": "1.3 Authentication\nLogin to your NASA Earthdata account and create a .netrc file using the login function from the earthaccess library. If you do not have an Earthdata Account, you can create one here.\n\n# authenticate\nearthaccess.login()\n\nEARTHDATA_USERNAME and EARTHDATA_PASSWORD are not set in the current environment, try setting them or use a different strategy (netrc, interactive)\nYou're now authenticated with NASA Earthdata Login\nUsing token with expiration date: 12/24/2023\nUsing .netrc file for EDL\n\n\n&lt;earthaccess.auth.Auth at 0x7fa6508d1a20&gt;"
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#open-a-gedi-hdf5-file-and-read-file-metadata",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#open-a-gedi-hdf5-file-and-read-file-metadata",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "3.1 Open a GEDI HDF5 File and Read File Metadata ",
    "text": "3.1 Open a GEDI HDF5 File and Read File Metadata \n\nRead the file using h5py.\n\nL2B = f'{data_dir}{gediFiles[0]}'\nL2B\n\n'/home/jovyan/shared/2023SSC/GEDI02_B_2022155015315_O19683_03_T05652_02_003_01_V002.h5'\n\n\n\n\nThe standard format for GEDI Version 2 filenames is as follows:\n\nGEDI02_B: Product Short Name\n2022159001702: Julian Date and Time of Acquisition (YYYYDDDHHMMSS)\nO19744: Orbit Number\n03: Sub-Orbit Granule Number (1-4)\nT08957: Track Number (Reference Ground Track)\n02: Positioning and Pointing Determination System (PPDS) type (00 is predict, 01 rapid, 02 and higher is final)\n003: PGE Version Number\n01: Granule Production Version\nV002: Product Version\n\n\n\nRead in a GEDI HDF5 file using the h5py package.\n\ngediL2B = h5py.File(L2B, 'r')  # Read file using h5py\n\n\n\nNavigate the HDF5 file below.\n\nlist(gediL2B.keys())\n\n['BEAM0000',\n 'BEAM0001',\n 'BEAM0010',\n 'BEAM0011',\n 'BEAM0101',\n 'BEAM0110',\n 'BEAM1000',\n 'BEAM1011',\n 'METADATA']\n\n\n\n\nThe GEDI HDF5 file contains groups in which data and metadata are stored.\n\n\nFirst, the METADATA group contains the file-level metadata.\n\nlist(gediL2B['METADATA'])\n\n['DatasetIdentification']\n\n\nThis contains useful information such as the creation date, PGEVersion, and VersionID. Below, print the file-level metadata attributes.\n\nfor g in gediL2B['METADATA']['DatasetIdentification'].attrs:\n    print(g, \": \", gediL2B['METADATA']['DatasetIdentification'].attrs[g]) \n\nPGEVersion :  003\nVersionID :  01\nabstract :  The GEDI L2B standard data product contains precise latitude, longitude, elevation, height, cover and vertical profile metrics for each laser footprint located on the land surface.\ncharacterSet :  utf8\ncreationDate :  2022-09-22T17:38:15.690108Z\ncredit :  The software that generates the L2B product was implemented within the GEDI Science Data Processing System at the NASA Goddard Space Flight Center (GSFC) in Greenbelt, Maryland in collaboration with the Department of Geographical Sciences at the University of Maryland (UMD).\nfileName :  GEDI02_B_2022155015315_O19683_03_T05652_02_003_01_V002.h5\nlanguage :  eng\noriginatorOrganizationName :  UMD/GSFC GEDI-SDPS &gt; GEDI Science Data Processing System\npurpose :  The purpose of the L2B dataset is to extract biophysical metrics from each GEDI waveform. These metrics are based on the directional gap probability profile derived from the L1B waveform and include canopy cover, Plant Area Index (PAI), Plant Area Volume Density (PAVD) and Foliage Height Diversity (FHD).\nshortName :  GEDI_L2B\nspatialRepresentationType :  along-track\nstatus :  onGoing\ntopicCategory :  geoscientificInformation\nuuid :  f7ecc77a-3372-4f53-9131-f110b4bbfb5c"
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#read-sds-metadata-and-subset-by-beam",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#read-sds-metadata-and-subset-by-beam",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "3.2 Read SDS Metadata and Subset by Beam ",
    "text": "3.2 Read SDS Metadata and Subset by Beam \n\nThe GEDI instrument consists of 3 lasers producing a total of 8 beam ground transects. The eight remaining groups contain data for each of the eight GEDI beam transects. For additional information, be sure to check out: https://gedi.umd.edu/instrument/specifications/.\n\nbeamNames = [g for g in gediL2B.keys() if g.startswith('BEAM')]\nbeamNames\n\n['BEAM0000',\n 'BEAM0001',\n 'BEAM0010',\n 'BEAM0011',\n 'BEAM0101',\n 'BEAM0110',\n 'BEAM1000',\n 'BEAM1011']\n\n\n\n\nOne useful piece of metadata to retrieve from each beam transect is whether it is a full power beam or a coverage beam.\n\nfor b in beamNames: \n    print(f\"{b} is a {gediL2B[b].attrs['description']}\")\n\nBEAM0000 is a Coverage beam\nBEAM0001 is a Coverage beam\nBEAM0010 is a Coverage beam\nBEAM0011 is a Coverage beam\nBEAM0101 is a Full power beam\nBEAM0110 is a Full power beam\nBEAM1000 is a Full power beam\nBEAM1011 is a Full power beam\n\n\n\n\nBelow, pick one of the full power beams that will be used to retrieve GEDI L2B shots in next section.\n\n\nIdentify all the objects in the GEDI HDF5 file below.\nNote: This step may take a while to complete.\n\ngediL2B_objs = []\ngediL2B.visit(gediL2B_objs.append)                                           # Retrieve list of datasets\ngediSDS = [o for o in gediL2B_objs if isinstance(gediL2B[o], h5py.Dataset)]  # Search for relevant SDS inside data file\n# gediSDS\n[i for i in gediSDS if beamNames[4] in i]                               # Print the datasets for a selected beam \n\n['BEAM0101/algorithmrun_flag',\n 'BEAM0101/ancillary/dz',\n 'BEAM0101/ancillary/l2a_alg_count',\n 'BEAM0101/ancillary/maxheight_cuttoff',\n 'BEAM0101/ancillary/rg_eg_constraint_center_buffer',\n 'BEAM0101/ancillary/rg_eg_mpfit_max_func_evals',\n 'BEAM0101/ancillary/rg_eg_mpfit_maxiters',\n 'BEAM0101/ancillary/rg_eg_mpfit_tolerance',\n 'BEAM0101/ancillary/signal_search_buff',\n 'BEAM0101/ancillary/tx_noise_stddev_multiplier',\n 'BEAM0101/beam',\n 'BEAM0101/channel',\n 'BEAM0101/cover',\n 'BEAM0101/cover_z',\n 'BEAM0101/fhd_normal',\n 'BEAM0101/geolocation/degrade_flag',\n 'BEAM0101/geolocation/delta_time',\n 'BEAM0101/geolocation/digital_elevation_model',\n 'BEAM0101/geolocation/elev_highestreturn',\n 'BEAM0101/geolocation/elev_lowestmode',\n 'BEAM0101/geolocation/elevation_bin0',\n 'BEAM0101/geolocation/elevation_bin0_error',\n 'BEAM0101/geolocation/elevation_lastbin',\n 'BEAM0101/geolocation/elevation_lastbin_error',\n 'BEAM0101/geolocation/height_bin0',\n 'BEAM0101/geolocation/height_lastbin',\n 'BEAM0101/geolocation/lat_highestreturn',\n 'BEAM0101/geolocation/lat_lowestmode',\n 'BEAM0101/geolocation/latitude_bin0',\n 'BEAM0101/geolocation/latitude_bin0_error',\n 'BEAM0101/geolocation/latitude_lastbin',\n 'BEAM0101/geolocation/latitude_lastbin_error',\n 'BEAM0101/geolocation/local_beam_azimuth',\n 'BEAM0101/geolocation/local_beam_elevation',\n 'BEAM0101/geolocation/lon_highestreturn',\n 'BEAM0101/geolocation/lon_lowestmode',\n 'BEAM0101/geolocation/longitude_bin0',\n 'BEAM0101/geolocation/longitude_bin0_error',\n 'BEAM0101/geolocation/longitude_lastbin',\n 'BEAM0101/geolocation/longitude_lastbin_error',\n 'BEAM0101/geolocation/shot_number',\n 'BEAM0101/geolocation/solar_azimuth',\n 'BEAM0101/geolocation/solar_elevation',\n 'BEAM0101/l2a_quality_flag',\n 'BEAM0101/l2b_quality_flag',\n 'BEAM0101/land_cover_data/landsat_treecover',\n 'BEAM0101/land_cover_data/landsat_water_persistence',\n 'BEAM0101/land_cover_data/leaf_off_doy',\n 'BEAM0101/land_cover_data/leaf_off_flag',\n 'BEAM0101/land_cover_data/leaf_on_cycle',\n 'BEAM0101/land_cover_data/leaf_on_doy',\n 'BEAM0101/land_cover_data/modis_nonvegetated',\n 'BEAM0101/land_cover_data/modis_nonvegetated_sd',\n 'BEAM0101/land_cover_data/modis_treecover',\n 'BEAM0101/land_cover_data/modis_treecover_sd',\n 'BEAM0101/land_cover_data/pft_class',\n 'BEAM0101/land_cover_data/region_class',\n 'BEAM0101/land_cover_data/urban_focal_window_size',\n 'BEAM0101/land_cover_data/urban_proportion',\n 'BEAM0101/master_frac',\n 'BEAM0101/master_int',\n 'BEAM0101/num_detectedmodes',\n 'BEAM0101/omega',\n 'BEAM0101/pai',\n 'BEAM0101/pai_z',\n 'BEAM0101/pavd_z',\n 'BEAM0101/pgap_theta',\n 'BEAM0101/pgap_theta_error',\n 'BEAM0101/pgap_theta_z',\n 'BEAM0101/rg',\n 'BEAM0101/rh100',\n 'BEAM0101/rhog',\n 'BEAM0101/rhog_error',\n 'BEAM0101/rhov',\n 'BEAM0101/rhov_error',\n 'BEAM0101/rossg',\n 'BEAM0101/rv',\n 'BEAM0101/rx_processing/algorithmrun_flag_a1',\n 'BEAM0101/rx_processing/algorithmrun_flag_a2',\n 'BEAM0101/rx_processing/algorithmrun_flag_a3',\n 'BEAM0101/rx_processing/algorithmrun_flag_a4',\n 'BEAM0101/rx_processing/algorithmrun_flag_a5',\n 'BEAM0101/rx_processing/algorithmrun_flag_a6',\n 'BEAM0101/rx_processing/pgap_theta_a1',\n 'BEAM0101/rx_processing/pgap_theta_a2',\n 'BEAM0101/rx_processing/pgap_theta_a3',\n 'BEAM0101/rx_processing/pgap_theta_a4',\n 'BEAM0101/rx_processing/pgap_theta_a5',\n 'BEAM0101/rx_processing/pgap_theta_a6',\n 'BEAM0101/rx_processing/pgap_theta_error_a1',\n 'BEAM0101/rx_processing/pgap_theta_error_a2',\n 'BEAM0101/rx_processing/pgap_theta_error_a3',\n 'BEAM0101/rx_processing/pgap_theta_error_a4',\n 'BEAM0101/rx_processing/pgap_theta_error_a5',\n 'BEAM0101/rx_processing/pgap_theta_error_a6',\n 'BEAM0101/rx_processing/rg_a1',\n 'BEAM0101/rx_processing/rg_a2',\n 'BEAM0101/rx_processing/rg_a3',\n 'BEAM0101/rx_processing/rg_a4',\n 'BEAM0101/rx_processing/rg_a5',\n 'BEAM0101/rx_processing/rg_a6',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a1',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a2',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a3',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a4',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a5',\n 'BEAM0101/rx_processing/rg_eg_amplitude_a6',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a1',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a2',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a3',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a4',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a5',\n 'BEAM0101/rx_processing/rg_eg_amplitude_error_a6',\n 'BEAM0101/rx_processing/rg_eg_center_a1',\n 'BEAM0101/rx_processing/rg_eg_center_a2',\n 'BEAM0101/rx_processing/rg_eg_center_a3',\n 'BEAM0101/rx_processing/rg_eg_center_a4',\n 'BEAM0101/rx_processing/rg_eg_center_a5',\n 'BEAM0101/rx_processing/rg_eg_center_a6',\n 'BEAM0101/rx_processing/rg_eg_center_error_a1',\n 'BEAM0101/rx_processing/rg_eg_center_error_a2',\n 'BEAM0101/rx_processing/rg_eg_center_error_a3',\n 'BEAM0101/rx_processing/rg_eg_center_error_a4',\n 'BEAM0101/rx_processing/rg_eg_center_error_a5',\n 'BEAM0101/rx_processing/rg_eg_center_error_a6',\n 'BEAM0101/rx_processing/rg_eg_chisq_a1',\n 'BEAM0101/rx_processing/rg_eg_chisq_a2',\n 'BEAM0101/rx_processing/rg_eg_chisq_a3',\n 'BEAM0101/rx_processing/rg_eg_chisq_a4',\n 'BEAM0101/rx_processing/rg_eg_chisq_a5',\n 'BEAM0101/rx_processing/rg_eg_chisq_a6',\n 'BEAM0101/rx_processing/rg_eg_flag_a1',\n 'BEAM0101/rx_processing/rg_eg_flag_a2',\n 'BEAM0101/rx_processing/rg_eg_flag_a3',\n 'BEAM0101/rx_processing/rg_eg_flag_a4',\n 'BEAM0101/rx_processing/rg_eg_flag_a5',\n 'BEAM0101/rx_processing/rg_eg_flag_a6',\n 'BEAM0101/rx_processing/rg_eg_gamma_a1',\n 'BEAM0101/rx_processing/rg_eg_gamma_a2',\n 'BEAM0101/rx_processing/rg_eg_gamma_a3',\n 'BEAM0101/rx_processing/rg_eg_gamma_a4',\n 'BEAM0101/rx_processing/rg_eg_gamma_a5',\n 'BEAM0101/rx_processing/rg_eg_gamma_a6',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a1',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a2',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a3',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a4',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a5',\n 'BEAM0101/rx_processing/rg_eg_gamma_error_a6',\n 'BEAM0101/rx_processing/rg_eg_niter_a1',\n 'BEAM0101/rx_processing/rg_eg_niter_a2',\n 'BEAM0101/rx_processing/rg_eg_niter_a3',\n 'BEAM0101/rx_processing/rg_eg_niter_a4',\n 'BEAM0101/rx_processing/rg_eg_niter_a5',\n 'BEAM0101/rx_processing/rg_eg_niter_a6',\n 'BEAM0101/rx_processing/rg_eg_sigma_a1',\n 'BEAM0101/rx_processing/rg_eg_sigma_a2',\n 'BEAM0101/rx_processing/rg_eg_sigma_a3',\n 'BEAM0101/rx_processing/rg_eg_sigma_a4',\n 'BEAM0101/rx_processing/rg_eg_sigma_a5',\n 'BEAM0101/rx_processing/rg_eg_sigma_a6',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a1',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a2',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a3',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a4',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a5',\n 'BEAM0101/rx_processing/rg_eg_sigma_error_a6',\n 'BEAM0101/rx_processing/rg_error_a1',\n 'BEAM0101/rx_processing/rg_error_a2',\n 'BEAM0101/rx_processing/rg_error_a3',\n 'BEAM0101/rx_processing/rg_error_a4',\n 'BEAM0101/rx_processing/rg_error_a5',\n 'BEAM0101/rx_processing/rg_error_a6',\n 'BEAM0101/rx_processing/rv_a1',\n 'BEAM0101/rx_processing/rv_a2',\n 'BEAM0101/rx_processing/rv_a3',\n 'BEAM0101/rx_processing/rv_a4',\n 'BEAM0101/rx_processing/rv_a5',\n 'BEAM0101/rx_processing/rv_a6',\n 'BEAM0101/rx_processing/rx_energy_a1',\n 'BEAM0101/rx_processing/rx_energy_a2',\n 'BEAM0101/rx_processing/rx_energy_a3',\n 'BEAM0101/rx_processing/rx_energy_a4',\n 'BEAM0101/rx_processing/rx_energy_a5',\n 'BEAM0101/rx_processing/rx_energy_a6',\n 'BEAM0101/rx_processing/shot_number',\n 'BEAM0101/rx_range_highestreturn',\n 'BEAM0101/rx_sample_count',\n 'BEAM0101/rx_sample_start_index',\n 'BEAM0101/selected_l2a_algorithm',\n 'BEAM0101/selected_mode',\n 'BEAM0101/selected_mode_flag',\n 'BEAM0101/selected_rg_algorithm',\n 'BEAM0101/sensitivity',\n 'BEAM0101/shot_number',\n 'BEAM0101/stale_return_flag',\n 'BEAM0101/surface_flag']\n\n\nThere are several datasets for each beam. View the GEDI L2B Dictionary for more details. You can also print the description for desired datasets.\n\nprint('pai: ', gediL2B['BEAM0101/pai'].attrs['description'])\nprint('pai_z: ', gediL2B['BEAM0101/pai_z'].attrs['description'])\nprint('pavd_z: ', gediL2B['BEAM0101/pavd_z'].attrs['description'])\nprint('shot_number: ', gediL2B['BEAM0101/geolocation/shot_number'].attrs['description'])\nprint('rh100: ', gediL2B['BEAM0101/rh100'].attrs['description'])\nprint('Quality Flag: ', gediL2B['BEAM0101/l2b_quality_flag'].attrs['description'])\n\npai:  Total plant area index\npai_z:  Vertical PAI profile from canopy height (z) to ground (z=0) with a vertical step size of dZ, where cover(z &gt; z_max) = 0\npavd_z:  Vertical Plant Area Volume Density profile with a vertical step size of dZ\nshot_number:  Unique shot ID.\nrh100:  Height above ground of the received waveform signal start (rh[101] from L2A)\nQuality Flag:  Flag simpilfying selection of most useful data for Level 2B\n\n\n\n\nWe will set the shot index used as an example from the GEDI L1B Tutorial and GEDI L2A Tutorial to show how to subset a single shot of GEDI L2B data."
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#subset-by-layer-and-create-a-geodataframe",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#subset-by-layer-and-create-a-geodataframe",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "4.1 Subset by Layer and Create a Geodataframe ",
    "text": "4.1 Subset by Layer and Create a Geodataframe \n\nRead in the SDS and take a representative sample (every 100th shot) and append to lists, then use the lists to generate a pandas dataframe.\n\nlonSample, latSample, shotSample, qualitySample, beamSample = [], [], [], [], []  # Set up lists to store data\n\n# Open the SDS\nlats = gediL2B[f'{beamNames[0]}/geolocation/lat_lowestmode'][()]\nlons = gediL2B[f'{beamNames[0]}/geolocation/lon_lowestmode'][()]\nshots = gediL2B[f'{beamNames[0]}/geolocation/shot_number'][()]\nquality = gediL2B[f'{beamNames[0]}/l2b_quality_flag'][()]\n\n# Take every 100th shot and append to list\nfor i in range(len(shots)):\n    if i % 100 == 0:\n        shotSample.append(str(shots[i]))\n        lonSample.append(lons[i])\n        latSample.append(lats[i])\n        qualitySample.append(quality[i])\n        beamSample.append(beamNames[0])\n            \n# Write all of the sample shots to a dataframe\nlatslons = pd.DataFrame({'Beam': beamSample, 'Shot Number': shotSample, 'Longitude': lonSample, 'Latitude': latSample,\n                         'Quality Flag': qualitySample})\nlatslons\n\n\n\n\n\n\n\n\nBeam\nShot Number\nLongitude\nLatitude\nQuality Flag\n\n\n\n\n0\nBEAM0000\n196830000300204328\n-150.407079\n51.480311\n0\n\n\n1\nBEAM0000\n196830000300204428\n-150.363476\n51.474715\n0\n\n\n2\nBEAM0000\n196830000300204528\n-150.322943\n51.470677\n0\n\n\n3\nBEAM0000\n196830000300204628\n-150.280632\n51.465678\n0\n\n\n4\nBEAM0000\n196830000300204728\n-150.239797\n51.461442\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n1532\nBEAM0000\n196830000300357528\n-77.574387\n0.617701\n0\n\n\n1533\nBEAM0000\n196830000300357628\n-77.544344\n0.575149\n0\n\n\n1534\nBEAM0000\n196830000300357728\n-77.514584\n0.532945\n0\n\n\n1535\nBEAM0000\n196830000300357828\n-77.484620\n0.490324\n0\n\n\n1536\nBEAM0000\n196830000300357928\n-77.454636\n0.447691\n0\n\n\n\n\n1537 rows √ó 5 columns\n\n\n\n\n\nAbove is a dataframe containing columns describing the beam, shot number, lat/lon location, and quality information about each shot.\n\n\nSide Note: Wondering what the 0‚Äôs and 1‚Äôs for l2b_quality_flag mean?\n\n\nAbove, 0 is poor quality and a quality_flag value of 1 indicates the laser shot meets criteria based on energy, sensitivity, amplitude, and real-time surface tracking quality. We will show an example of how to quality filter GEDI data.\n\n# Clean up variables that will no longer be needed\n# del beamSample, quality, qualitySample, gediL2B_objs, latSample, lats, lonSample, lons, shotSample, shots \n\n\n\nBelow, create an additional column called ‚Äògeometry‚Äô that contains a shapely point generated from each lat/lon location from the shot.\n\n# Take the lat/lon dataframe and convert each lat/lon to a shapely point\nlatslons['geometry'] = latslons.apply(lambda row: Point(row.Longitude, row.Latitude), axis=1)\n\n\n\nNext, convert to a Geopandas GeoDataFrame.\n\n# Convert to a Geodataframe\nlatslons = gp.GeoDataFrame(latslons)\nlatslons = latslons.drop(columns=['Latitude','Longitude'])\nlatslons['geometry']\n\n0       POINT (-150.40708 51.48031)\n1       POINT (-150.36348 51.47471)\n2       POINT (-150.32294 51.47068)\n3       POINT (-150.28063 51.46568)\n4       POINT (-150.23980 51.46144)\n                   ...             \n1532      POINT (-77.57439 0.61770)\n1533      POINT (-77.54434 0.57515)\n1534      POINT (-77.51458 0.53295)\n1535      POINT (-77.48462 0.49032)\n1536      POINT (-77.45464 0.44769)\nName: geometry, Length: 1537, dtype: geometry\n\n\n\n\nPull out and plot an example shapely point below."
  },
  {
    "objectID": "tutorials/GEDI_L2B_V2_earthaccess.html#visualize-a-geodataframe",
    "href": "tutorials/GEDI_L2B_V2_earthaccess.html#visualize-a-geodataframe",
    "title": "Getting Started with GEDI L2B Version 2 Data in Python",
    "section": "4.2 Visualize a GeoDataFrame ",
    "text": "4.2 Visualize a GeoDataFrame \n\nIn this section, use the GeoDataFrame and the geoviews python package to spatially visualize the location of the GEDI shots on a basemap and import a GeoJSON file of the spatial region of interest for the use case example: Redwood National Park.\n\n# Define a function for visualizing GEDI points\ndef pointVisual(features, vdims):\n    return (gvts.EsriImagery * gv.Points(features, vdims=vdims).options(tools=['hover'], height=500, width=900, size=5, \n                                                                        color='yellow', fontsize={'xticks': 10, 'yticks': 10, \n                                                                                                  'xlabel':16, 'ylabel': 16}))\n\n\n\nImport a GeoJSON of Reserva de la Bi√≥sfera Calakmul National Park as an additional GeoDataFrame.\n\ncalakmul = gp.GeoDataFrame.from_file(f'{data_dir}calakmul.geojson')  # Import GeoJSON as GeoDataFrame\n\n\ncalakmul\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOLYGON ((-89.00000 19.00000, -91.50000 19.000...\n\n\n\n\n\n\n\n\ncalakmul['geometry'][0]  # Plot GeoDataFrame\n\n\n\n\n\n\nDefining the vdims below will allow you to hover over specific shots and view information about them.\n\n# Create a list of geodataframe columns to be included as attributes in the output map\nvdims = []\nfor f in latslons:\n    if f not in ['geometry']:\n        vdims.append(f)\nvdims\n\n['Beam', 'Shot Number', 'Quality Flag']\n\n\n\n\nBelow, combine a plot of the Redwood National Park Boundary (combine two geoviews plots using *) with the point visual mapping function defined above in order to plot (1) the representative GEDI shots, (2) the region of interest, and (3) a basemap layer.\n\n# Call the function for plotting the GEDI points\ngv.Polygons(calakmul['geometry']).opts(line_color='red', color=None) * pointVisual(latslons, vdims = vdims)\n\n\n\n\n\n  \n\n\n\n\n\n\nEach GEDI shot has a unique shot identifier (shot number) that is available within each data group of the product. The shot number is important to retain in any data subsetting as it will allow the user to link any shot record back to the original orbit data, and to link any shot and its data between the L1 and L2 products. The standard format for GEDI Shots is as follows:"
  },
  {
    "objectID": "tutorials/data-access/index.html",
    "href": "tutorials/data-access/index.html",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "",
    "text": "Outline: * ICESat-2 Data Access, NASA DAAC @ NSIDC * Overview of ICESat-2 Mission and land products (see USFS Application Workshop slides * NSIDC DAAC resources / tools * Walk through myriad options with different capabilities/use cases * Modified version of table from IS2 Hackweek * Basic earthaccess demo of ATL08 access * Need common area / time of interest across IS2 / GEDI tutorials\n\nThe icepyx Software Library and Community\n\n\n\nThis notebook demonstrates searching for cloud-hosted ICESat-2 data and directly accessing Land Ice Height (ATL06) granules from an Amazon Compute Cloud (EC2) instance using the earthaccess package. NASA data ‚Äúin the cloud‚Äù are stored in Amazon Web Services (AWS) Simple Storage Service (S3) Buckets. Direct Access is an efficient way to work with data stored in an S3 Bucket when you are working in the cloud. Cloud-hosted granules can be opened and loaded into memory without the need to download them first. This allows you take advantage of the scalability and power of cloud computing.\nThe Amazon Global cloud is divided into geographical regions. To have direct access to data stored in a region, our compute instance - a virtual computer that we create to perform processing operations in place of using our own desktop or laptop - must be in the same region as the data. This is a fundamental concept of analysis in place. NASA cloud-hosted data is in Amazon Region us-west2. So your compute instance must also be in us-west2. If we wanted to use data stored in another region, to use direct access for that data, we would start a compute instance in that region.\nAs an example data collection, we use ICESat-2 Land Ice Height (ATL06) over the Juneau Icefield, AK, for March 2003. ICESat-2 data granules, including ATL06, are stored in HDF5 format. We demonstrate how to open an HDF5 granule and access data variables using xarray. Land Ice Heights are then plotted using hvplot.\nearthaccess is a package developed by Luis Lopez (NSIDC developer) to allow easy search of the NASA Common Metadata Repository (CMR) and download of NASA data collections. It can be used for programmatic search and access for both DAAC-hosted and cloud-hosted data. It manages authenticating using Earthdata Login credentials which are then used to obtain the S3 tokens that are needed for S3 direct access. https://github.com/nsidc/earthaccess\n\n\n\nThe notebook was created by ‚Ä¶\n\n\n\nBy the end of this demonstration you will be able to:\n1. use earthaccess to search for ICESat-2 data using spatial and temporal filters and explore search results;\n2. open data granules using direct access to the ICESat-2 S3 bucket;\n3. load a HDF5 group into an xarray.Dataset;\n4. visualize the land ice heights using hvplot.\n\n\n\nSee Prerequisites"
  },
  {
    "objectID": "tutorials/data-access/index.html#tutorial-overview",
    "href": "tutorials/data-access/index.html#tutorial-overview",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "",
    "text": "Outline: * ICESat-2 Data Access, NASA DAAC @ NSIDC * Overview of ICESat-2 Mission and land products (see USFS Application Workshop slides * NSIDC DAAC resources / tools * Walk through myriad options with different capabilities/use cases * Modified version of table from IS2 Hackweek * Basic earthaccess demo of ATL08 access * Need common area / time of interest across IS2 / GEDI tutorials\n\nThe icepyx Software Library and Community\n\n\n\nThis notebook demonstrates searching for cloud-hosted ICESat-2 data and directly accessing Land Ice Height (ATL06) granules from an Amazon Compute Cloud (EC2) instance using the earthaccess package. NASA data ‚Äúin the cloud‚Äù are stored in Amazon Web Services (AWS) Simple Storage Service (S3) Buckets. Direct Access is an efficient way to work with data stored in an S3 Bucket when you are working in the cloud. Cloud-hosted granules can be opened and loaded into memory without the need to download them first. This allows you take advantage of the scalability and power of cloud computing.\nThe Amazon Global cloud is divided into geographical regions. To have direct access to data stored in a region, our compute instance - a virtual computer that we create to perform processing operations in place of using our own desktop or laptop - must be in the same region as the data. This is a fundamental concept of analysis in place. NASA cloud-hosted data is in Amazon Region us-west2. So your compute instance must also be in us-west2. If we wanted to use data stored in another region, to use direct access for that data, we would start a compute instance in that region.\nAs an example data collection, we use ICESat-2 Land Ice Height (ATL06) over the Juneau Icefield, AK, for March 2003. ICESat-2 data granules, including ATL06, are stored in HDF5 format. We demonstrate how to open an HDF5 granule and access data variables using xarray. Land Ice Heights are then plotted using hvplot.\nearthaccess is a package developed by Luis Lopez (NSIDC developer) to allow easy search of the NASA Common Metadata Repository (CMR) and download of NASA data collections. It can be used for programmatic search and access for both DAAC-hosted and cloud-hosted data. It manages authenticating using Earthdata Login credentials which are then used to obtain the S3 tokens that are needed for S3 direct access. https://github.com/nsidc/earthaccess\n\n\n\nThe notebook was created by ‚Ä¶\n\n\n\nBy the end of this demonstration you will be able to:\n1. use earthaccess to search for ICESat-2 data using spatial and temporal filters and explore search results;\n2. open data granules using direct access to the ICESat-2 S3 bucket;\n3. load a HDF5 group into an xarray.Dataset;\n4. visualize the land ice heights using hvplot.\n\n\n\nSee Prerequisites"
  },
  {
    "objectID": "tutorials/data-access/index.html#tutorial-steps",
    "href": "tutorials/data-access/index.html#tutorial-steps",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "2. Tutorial steps",
    "text": "2. Tutorial steps"
  },
  {
    "objectID": "tutorials/data-access/index.html#import-packages",
    "href": "tutorials/data-access/index.html#import-packages",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Import Packages",
    "text": "Import Packages\nThe first step in any python script or notebook is to import packages. This tutorial requires the following packages: - earthaccess, which enables Earthdata Login authentication and retrieves AWS credentials; enables collection and granule searches; and S3 access; - xarray, used to load data; - hvplot, used to visualize land ice height data.\nWe are going to import the whole earthaccess package.\nWe will also import the whole xarray package but use a standard short name xr, using the import &lt;package&gt; as &lt;short_name&gt; syntax. We could use anything for a short name but xr is an accepted standard that most xarray users are familiar with.\nWe only need the xarray module from hvplot so we import that using the import &lt;package&gt;.&lt;module&gt; syntax.\n\n# For searching NASA data\nimport earthaccess\n\n# For reading data, analysis and plotting\nimport xarray as xr\nimport hvplot.xarray\nimport pprint"
  },
  {
    "objectID": "tutorials/data-access/index.html#authenticate",
    "href": "tutorials/data-access/index.html#authenticate",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Authenticate",
    "text": "Authenticate\nThe first step is to get the correct authentication that will allow us to get cloud-hosted ICESat-2 data. This is all done through Earthdata Login. The login method also gets the correct AWS credentials.\nLogin requires your Earthdata Login username and password. The login method will automatically search for these credentials as environment variables or in a .netrc file, and if those aren‚Äôt available it will prompt us to enter our username and password. We use a .netrc strategy. A .netrc file is a text file located in our home directory that contains login information for remote machines. If we don‚Äôt have a .netrc file, login can create one for us.\nearthaccess.login(strategy='interactive', persist=True)\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "tutorials/data-access/index.html#search-for-icesat-2-collections",
    "href": "tutorials/data-access/index.html#search-for-icesat-2-collections",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Search for ICESat-2 Collections",
    "text": "Search for ICESat-2 Collections\nearthaccess leverages the Common Metadata Repository (CMR) API to search for collections and granules. Earthdata Search also uses the CMR API.\nWe can use the search_datasets method to search for ICESat-2 collections by setting keyword='ICESat-2'.\nThis will display the number of data collections (data sets) that meet this search criteria.\n\nQuery = earthaccess.search_datasets(keyword = 'ICESat-2')\n\nIn this case there are 65 collections that have the keyword ICESat-2.\nThe search_datasets method returns a python list of DataCollection objects. We can view the metadata for each collection in long form by passing a DataCollection object to print or as a summary using the summary method. We can also use the pprint function to Pretty Print each object.\nWe will do this for the first 10 results (objects).\n\nfor collection in Query[:10]:\n    pprint.pprint(collection.summary(), sort_dicts=True, indent=4)\n    print('')\n    \n\nFor each collection, summary returns a subset of fields from the collection metadata and the Unified Metadata Model (UMM): - concept-id is a unique id for the collection. It consists of an alphanumeric code and the provider-id specific to the DAAC (Distributed Active Archive Center). You can use the concept_id to search for data granules. - short_name is a quick way of referring to a collection (instead of using the full title). It can be found on the collection landing page underneath the collection title after ‚ÄòDATA SET ID‚Äô. See the table below for a list of the shortnames for ICESat-2 collections. - version is the version of each collection. - file-type gives information about the file format of the collection granules. - get-data is a collection of URLs that can be used to access the data, collection landing pages and data tools. - cloud-info this is for cloud-hosted data and provides additional information about the location of the S3 bucket that holds the data and where to get temporary AWS S3 credentials to access the S3 buckets. earthaccess handles these credentials and the links to the S3 buckets, so in general you won‚Äôt need to worry about this information.\nFor the ICESat-2 search results, within the concept-id, there is a provider-id; NSIDC_ECS and NSIDC_CPRD. NSIDC_ECS which is for the on-prem collections and NSIDC_CPRD is for the cloud-hosted collections.\nFor ICESat-2, ShortNames are generally how different products are referred to.\n\n\n\n\n\n\n\nShortName\nProduct Description\n\n\n\n\nATL03\nATLAS/ICESat-2 L2A Global Geolocated Photon Data\n\n\nATL06\nATLAS/ICESat-2 L3A Land Ice Height\n\n\nATL07\nATLAS/ICESat-2 L3A Sea Ice Height\n\n\nATL08\nATLAS/ICESat-2 L3A Land and Vegetation Height\n\n\nATL09\nATLAS/ICESat-2 L3A Calibrated Backscatter Profiles and Atmospheric Layer Characteristics\n\n\nATL10\nATLAS/ICESat-2 L3A Sea Ice Freeboard\n\n\nATL11\nATLAS/ICESat-2 L3B Slope-Corrected Land Ice Height Time Series\n\n\nATL12\nATLAS/ICESat-2 L3A Ocean Surface Height\n\n\nATL13\nATLAS/ICESat-2 L3A Along Track Inland Surface Water Data\n\n\n\n\nSearch for cloud-hosted data\nFor most collections, to search for only data in the cloud, the cloud_hosted method can be used.\n\nQuery = earthaccess.search_datasets(\n    keyword = 'ICESat-2',\n    cloud_hosted = True\n)"
  },
  {
    "objectID": "tutorials/data-access/index.html#search-a-data-set-using-spatial-and-temporal-filters",
    "href": "tutorials/data-access/index.html#search-a-data-set-using-spatial-and-temporal-filters",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Search a data set using spatial and temporal filters",
    "text": "Search a data set using spatial and temporal filters\nWe can use the search_data method to search for granules within a data set by location and time using spatial and temporal filters. In this example, we will search for data granules from the ATL06 verison 006 cloud-hosted data set over the Juneau Icefield, AK, for March and April 2020.\nThe temporal range is identified with standard date strings, and latitude-longitude corners of a bounding box is specified. Polygons and points, as well as shapefiles can also be specified.\nThis will display the number of granules that match our search.\n\nresults = earthaccess.search_data(\n    short_name = 'ATL06',\n    version = '006',\n    cloud_hosted = True,\n    bounding_box = (-134.7,58.9,-133.9,59.2),\n    temporal = ('2020-03-01','2020-04-30'),\n)\n\nTo display the rendered metadata, including the download link, granule size and two images, we will use display. In the example below, all 4 results are shown.\nThe download link is https and can be used download the granule to your local machine. This is similar to downloading DAAC-hosted data but in this case the data are coming from the Earthdata Cloud. For NASA data in the Earthdata Cloud, there is no charge to the user for egress from AWS Cloud servers. This is not the case for other data in the cloud.\nNote the [None, None, None, None] that is displayed at the end can be ignored, it has no meaning in relation to the metadata.\n\n[display(r) for r in results]"
  },
  {
    "objectID": "tutorials/data-access/index.html#use-direct-access-to-open-load-and-display-data-stored-on-s3",
    "href": "tutorials/data-access/index.html#use-direct-access-to-open-load-and-display-data-stored-on-s3",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "Use Direct-Access to open, load and display data stored on S3",
    "text": "Use Direct-Access to open, load and display data stored on S3\nDirect-access to data from an S3 bucket is a two step process. First, the files are opened using the open method. The auth object created at the start of the notebook is used to provide Earthdata Login authentication and AWS credentials.\nThe next step is to load the data. In this case, data are loaded into an xarray.Dataset. Data could be read into numpy arrays or a pandas.Dataframe. However, each granule would have to be read using a package that reads HDF5 granules such as h5py. xarray does this all under-the-hood in a single line but for a single group in the HDF5 granule*.\n*ICESat-2 measures photon returns from 3 beam pairs numbered 1, 2 and 3 that each consist of a left and a right beam. In this case, we are interested in the left ground track (gt) of beam pair 1.\n\nfiles = earthaccess.open(results)\nds = xr.open_dataset(files[1], group='/gt1l/land_ice_segments')\n\n\nds\n\nhvplot is an interactive plotting tool that is useful for exploring data.\n\nds['h_li'].hvplot(kind='scatter', s=2)"
  },
  {
    "objectID": "tutorials/data-access/index.html#learning-outcomes-recap",
    "href": "tutorials/data-access/index.html#learning-outcomes-recap",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "3. Learning outcomes recap",
    "text": "3. Learning outcomes recap\nWe have learned how to: 1. use earthaccess to search for ICESat-2 data using spatial and temporal filters and explore search results; 2. open data granules using direct access to the ICESat-2 S3 bucket; 3. load a HDF5 group into an xarray.Dataset; 4. visualize the land ice heights using hvplot."
  },
  {
    "objectID": "tutorials/data-access/index.html#additional-resources",
    "href": "tutorials/data-access/index.html#additional-resources",
    "title": "Accessing and working with ICESat-2 data in the cloud",
    "section": "4. Additional resources",
    "text": "4. Additional resources\nFor general information about NSIDC DAAC data in the Earthdata Cloud:\nFAQs About NSIDC DAAC‚Äôs Earthdata Cloud Migration\nNASA Earthdata Cloud Data Access Guide\nAdditional tutorials and How Tos:\nNASA Earthdata Cloud Cookbook"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "",
    "text": "In this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each variable.\nWe will access a single COG file, Land Surface Temperature (LST), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#summary",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#summary",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "",
    "text": "In this notebook, we will access data for the ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each variable.\nWe will access a single COG file, Land Surface Temperature (LST), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data."
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#requirements",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#requirements",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Requirements",
    "text": "Requirements\n\n1. AWS instance running in us-west-2\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n3. netrc File\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata."
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#learning-objectives",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#learning-objectives",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECOSTRESS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\nImport Packages\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport xarray as xr\nimport geopandas\nfrom shapely.geometry import Polygon\nfrom shapely.ops import transform\nimport pyproj\nfrom pyproj import Proj\nimport hvplot.xarray\nimport holoviews as hv\nimport geoviews as gv\ngv.extension('bokeh', 'matplotlib')"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#get-temporary-aws-credentials",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#get-temporary-aws-credentials",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Get Temporary AWS Credentials",
    "text": "Get Temporary AWS Credentials\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#workspace-environment-setup",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#workspace-environment-setup",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Workspace Environment Setup",
    "text": "Workspace Environment Setup\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We‚Äôll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL‚Äôs ‚ÄúVirtual File Systems‚Äù to read remote files. GDAL has a lot of environment variables that control it‚Äôs behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we‚Äôre interested in the ECOSTRESS Tiled Land Surface Temperature and Emissivity data collection from NASA‚Äôs LP DAAC in Earthdata Cloud. Below we specify the S3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\n#s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020272T183449.v2.0/HLS.L30.T10SGD.2020272T183449.v2.0.B04.tif'\ns3_url_lst = 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif'\ns3_url_qa = 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif'"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#direct-in-region-access",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#direct-in-region-access",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Direct In-region Access",
    "text": "Direct In-region Access\nRead in the ECOSTRESS Tiles LST S3 URL into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url_lst, chunks='auto')\nda\n\n\nqa = rioxarray.open_rasterio(s3_url_qa, chunks='auto')\nqa\n\n\nLST_dataset = xr.Dataset({'LST': da, 'quality': qa})\nLST_dataset\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we‚Äôll use the squeeze() function to remove band as a dimension.\n\nda_lst = LST_dataset.squeeze('band', drop=True)\nda_lst\n\nPlot the dataarray, representing the LST, using hvplot.\n\nda_lst['LST'].hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, tiles='EsriImagery', width=800, height=600, colorbar=True, title = 'Land Surface Temperature')"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#define-the-region-of-interest-for-clipping",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#define-the-region-of-interest-for-clipping",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Define the Region of Interest for Clipping",
    "text": "Define the Region of Interest for Clipping\nWe‚Äôll read in our GeoJSON file of our points of interest and create bounding box that contains a points coordinates\n\nfield = geopandas.read_file('../../tutorials/landcover.geojson')\n\nExtract the min/max values for the y and x axis\n\nminx, miny, maxx, maxy = field.geometry.total_bounds\nminx, miny, maxx, maxy\n\nOrder the coordinates for the bounding box counterclockwise\n\ncoords = [\n    (minx, miny),\n    (maxx, miny),\n    (maxx, maxy),\n    (minx, maxy)\n]\n\nCreate a shapely polygon\n\nfeature_shape = Polygon(coords)\nfeature_shape\n\n\nbase = gv.tile_sources.EsriImagery.opts(width=700, height=500)\nfarmField = gv.Polygons(feature_shape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField \n\nLet‚Äôs take a look at the bounding coordinate values.\nNote, the values above are in decimal degrees and represent the longitude and latitude for the lower left corner and upper right corner respectively.\n\nfeature_shape.bounds\n\nGet the projection information from the ECOSTRESS file\n\nsrc_proj = da_lst.rio.crs\nsrc_proj\n\nTransform coordinates from lat lon (units = dd) to UTM (units = m)\n\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\n\n\nproject = pyproj.Transformer.from_proj(geo_CRS, src_proj)                    # Set up the transformation\n\n\nfsUTM = transform(project.transform, feature_shape)\nfsUTM.bounds\n\nThe coordinates for our feature have now been converted to source raster projection. Note the difference in the values between feature_shape.bounds (in geographic) and fsUTM.bounds (in UTM projection).\nNow we can clip our ECOSTRESS LST file to our region of insterest!"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#access-and-clip-the-ecostress-lst-cog",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#access-and-clip-the-ecostress-lst-cog",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Access and clip the ECOSTRESS LST COG",
    "text": "Access and clip the ECOSTRESS LST COG\nWe can now use our transformed ROI bounding box to clip the ECOSTRESS S3 object we accessed before. We‚Äôll use the rio.clip\n\nda_lst_clip = rioxarray.open_rasterio(s3_url_lst, chunks='auto').squeeze('band', drop=True).rio.clip([fsUTM])\n\n\nda_lst_clip\n\n\nda_lst_clip.hvplot.image(x = 'x', y = 'y', crs = 'EPSG:32610', cmap='jet', rasterize=False, width=800, height=600,title = 'Land Surface Temperature (Kelvin)', colorbar=True)\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#resources",
    "href": "how-tos/data-access/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html#resources",
    "title": "Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access",
    "section": "Resources",
    "text": "Resources\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html",
    "title": "Data discovery with NASA‚Äôs CMR",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for ECOSTRESS version 2 collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#summary",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#summary",
    "title": "Data discovery with NASA‚Äôs CMR",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for ECOSTRESS version 2 collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#learning-objectives",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#learning-objectives",
    "title": "Data discovery with NASA‚Äôs CMR",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand what CMR/CMR API is and what CMR/CMR API can be used for\nHow to use the requests package to search data collections and granules\nHow to parse the results of these searches."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#what-is-cmr",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#what-is-cmr",
    "title": "Data discovery with NASA‚Äôs CMR",
    "section": "What is CMR",
    "text": "What is CMR\nCMR is the Common Metadata Repository. It catalogs all data for NASA‚Äôs Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#what-is-the-cmr-api",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#what-is-the-cmr-api",
    "title": "Data discovery with NASA‚Äôs CMR",
    "section": "What is the CMR API",
    "text": "What is the CMR API\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results."
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#getting-started-how-to-search-cmr-from-python",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#getting-started-how-to-search-cmr-from-python",
    "title": "Data discovery with NASA‚Äôs CMR",
    "section": "Getting Started: How to search CMR from Python",
    "text": "Getting Started: How to search CMR from Python\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in-depth tutorial on requests is here\n\nimport requests\nimport json\nfrom pprint import pprint\n\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We‚Äôll assign this url to a python variable as a string.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for collections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the end of the CMR endpoint URL.\nWe are going to search collections first, so we add \"collections\" to the URL. We are using a python format string in the examples below.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\nurl\n\n'https://cmr.earthdata.nasa.gov/search/collections'\n\n\nIn this first example, we want to retrieve a list of ECOSTRESS collections in the Earthdata Cloud. This includes ECOSTRESS collections for built 7.1 data which recently became publicly available. This means you will not need to generate a token to access data. Before the public release you should have been part of the access list to access the data. Because of that, an extra token parameter, generated using your Earthdata Login credentials needed to be passed in each CMR request that indicated you are a valid user.\nwe want to retrieve the collections that are hosted in the cloud ('cloud_hosted': 'True') that has granules availble ('has_granules': 'True'). We also want to get the content in json (pronounced ‚Äújason‚Äù) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json ('Accept': 'application/json').\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nThe request returns a Response object.\nTo check that our request was successful we can print the response variable we saved the request to.\n\nresponse\n\n&lt;Response [200]&gt;\n\n\nA 200 response is what we want. This means that the requests was successful. For more information on HTTP status codes see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nA more explict way to check the status code is to use the status_code attribute. Both methods return a HTTP status code.\n\nresponse.status_code\n\n200\n\n\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. We requested (above) that the information be return in json which means the object return is a dictionary in our Python environment. We‚Äôll iterate through the returned dictionary, looping throught each field (k) and its associated value (v). For more on interating through dictionary object click here.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nContent-Type: application/json;charset=utf-8\nContent-Length: 4204\nConnection: keep-alive\nDate: Tue, 15 Nov 2022 18:08:14 GMT\nX-Frame-Options: SAMEORIGIN\nAccess-Control-Allow-Origin: *\nX-XSS-Protection: 1; mode=block\nCMR-Request-Id: cf80d8ad-a428-4ca4-a85e-998ec5b0c02f\nStrict-Transport-Security: max-age=31536000\nCMR-Search-After: [0.0,23600.0,\"SENTINEL-1A_DP_META_GRD_HIGH\",\"1\",1214470576,826]\nCMR-Hits: 1674\nAccess-Control-Expose-Headers: CMR-Hits, CMR-Request-Id, X-Request-Id, CMR-Scroll-Id, CMR-Search-After, CMR-Timed-Out, CMR-Shapefile-Original-Point-Count, CMR-Shapefile-Simplified-Point-Count\nX-Content-Type-Options: nosniff\nCMR-Took: 237\nX-Request-Id: RjtJpW50AJUR058fFO1oB1ULN3tF72Vo-ffvb9N7FDYx7GjZPHVE1w==\nVary: Accept-Encoding, User-Agent\nContent-Encoding: gzip\nServer: ServerTokens ProductOnly\nX-Cache: Miss from cloudfront\nVia: 1.1 aa0280f933863b8ffd5ff636330f4170.cloudfront.net (CloudFront)\nX-Amz-Cf-Pop: HIO50-C2\nX-Amz-Cf-Id: RjtJpW50AJUR058fFO1oB1ULN3tF72Vo-ffvb9N7FDYx7GjZPHVE1w==\n\n\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but the keys uniquely case-insensitive. Let‚Äôs take a look at the commonly used CMR-Hits key.\n\nresponse.headers['CMR-Hits']\n\n'1674'\n\n\nNote that ‚Äúcmr-hits‚Äù works as well!\n\nresponse.headers['cmr-hits']\n\n'1674'\n\n\nIn some situations the response to your query can return a very large number of result, some of which may not be relevant. We can add additional query parameters to restrict the information returned. We‚Äôre going to restrict the search by the provider parameter.\nYou can modify the code below to explore all Earthdata data products hosted by the various providers. When searching by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA‚Äôs Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA‚Äôs Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWe‚Äôll assign the provider to a variable as a string and insert the variable into the parameter argument in the request. We‚Äôll also assign the term ‚ÄòECOSTRESS‚Äô to a varible so we don‚Äôt need to repeatedly add it to the requests parameters.\n\nprovider = 'LPCLOUD'\nproject = 'ECOSTRESS'\n\n\nheaders = {\n    #'Authorization': f'Bearer {token}',\n    'Accept': 'application/json',\n}\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'project': project,\n                        },\n                        headers=headers\n                       )\nresponse\n\n&lt;Response [200]&gt;\n\n\n\nresponse.headers['cmr-hits']\n\n'5'\n\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nb'{\"feed\":{\"updated\":\"2022-11-15T18:08:14.505Z\",\"id\":\"https://cmr.earthdata.nasa.gov:443/search/collections.json?cloud_hosted=True&has_granules=True&provider=LPCLOUD&project=ECOSTRESS\",\"title\":\"ECHO dataset metadata\",\"entry\":[{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"updated\":\"2021-06-23T16:50:51.108Z\",\"dataset_id\":\"ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2T_LSTE\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website.\\\\r\\\\nThe ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous Level 2 Global 70 m (ECO_L2T_LSTE) Version 2 data product provides atmospherically corrected land surface temperature and emissivity (LST&E) values derived from five thermal infrared (TIR) bands. The ECO_L2T_LSTE data product was derived using a physics-based Temperature/Emissivity Separation (TES) algorithm. This tiled data product is subset from the ECO_L2G_LSTE data product using a modified version of the Military Grid Reference System (MGRS) which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution.\\\\r\\\\nThe ECO_L2T_LSTE Version 2 data product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate COG. This product contains seven layers including LST, LST error, wideband emissivity, quality flags, height, and cloud and water masks. For acquisitions after May 15, 2019, data products contain data values for TIR bands 2, 4, and 5 only. TIR bands 1 and 3 contain fill values to accommodate direct streaming of data from the ISS, as mentioned in the Known Issues section. LST data generated after May 15, 2019 will only use the 3 available bands, accuracy may be affected when compared to the LST data that utilized all 5 bands.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076090826-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":true,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/browse#\",\"hreflang\":\"en-US\",\"href\":\"https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01.png\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf\"}]},{\"processing_level_id\":\"1B\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L1B_GEO\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected  FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Geolocation Instantaneous Level 1B Global (ECO_L1B_GEO) Version 2 data product provides the geolocation information for the radiance values retrieved in the ECO_L1B_RAD (https://doi.org/10.5067/ecostress/eco_l1b_rad.002) Version 2 data product. The geolocation product gives geo-tagging to each of the radiance pixels. The geolocation processing corrects the ISS-reported ephemeris and attitude data by image matching with a global ortho-base derived from Landsat data, and then assigns latitude and longitude values to each of the Level 1 radiance pixels. When image matching is successful, the data are geolocated to better than 50 meter (m) accuracy. The ECO_L1B_GEO data product is provided as swath data.\\\\r\\\\n\\\\r\\\\nThe ECO_L1B_GEO data product contains data layers for latitude and longitude values, solar and view geometry information, surface height, and the fraction of pixel on land versus water distributed in HDF5 format.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076087338-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076087338-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1491/ECO1B_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/223/ECO1B_Geolocation_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/222/ECO1B_Calibration_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1321/ECO1B_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/225/ECO1B_Rad_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/226/ECO1B_Geo_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"},{\"length\":\"700.0MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"}]},{\"processing_level_id\":\"1B\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L1B_RAD\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected  FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m (ECO_L1B_RAD) Version 2 data product provides at-sensor calibrated radiance values retrieved for five thermal infrared (TIR) bands operating between 8 and 12.5 \\xc2\\xb5m. Additionally, the digital numbers (DN) for the shortwave infrared (SWIR) band are provided. The TIR bands are spatially co-registered to produce a variable spatial resolution between 70 meters (m) and 90 m at the edge of the swath. The ECO_L1B_RAD data product is provided as swath data and does not contain geolocation information. The corresponding ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L1B_RAD data product. The geographic coverage of acquisitions for the ECO_L1B_RAD Version 2 data product extends to areas outside of those indicated on the coverage map. \\\\r\\\\n\\\\r\\\\nThe ECO_L1B_RAD Version 2 data product contains layers of radiance values for the five TIR bands, DN values for the SWIR band, associated data quality indicators, and ancillary data distributed in HDF5 format. For acquisitions after May 15, 2019, data products contain data values for the 8.785 \\xce\\xbcm, 10.522 \\xce\\xbcm, and 12.001 \\xce\\xbcm (TIR) bands only. The 1.6 \\xce\\xbcm (SWIR), 8.285 \\xce\\xbcm (TIR), and 9.060 \\xce\\xbcm (TIR) bands contain fill values to accommodate direct streaming of data from the ISS.\\\\r\\\\n\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076116385-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076116385-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L1B_RAD.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1491/ECO1B_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/223/ECO1B_Geolocation_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/222/ECO1B_Calibration_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1321/ECO1B_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/225/ECO1B_Rad_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/226/ECO1B_Geo_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"}]},{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2_CLOUD\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m (ECO_L2_CLOUD) Version 2 data product is derived using a single-channel Bayesian cloud threshold with a look-up-table (LUT) approach. The ECOSTRESS Level 2 cloud product provides a cloud mask that can be used to determine cloud cover for accurate land surface temperature and evapotranspiration estimation. The corresponding ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L2_CLOUD data product.\\\\r\\\\n \\\\r\\\\nThe ECO_L2_CLOUD Version 2 data product contains three cloud mask layers: brightness temperature LUT test, brightness temperature difference test, and final cloud mask. Information on how to interpret the bit fields in the cloud mask is provided in Table 7 of the User Guide (https://lpdaac.usgs.gov/documents/1493/ECOL2_User_Guide_V2.pdf).\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076115306-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076115306-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2_CLOUD.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1495/ECOL2_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1496/ECOL2_ATBD_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"},{\"length\":\"1.5MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"}]},{\"processing_level_id\":\"2\",\"cloud_hosted\":true,\"boxes\":[\"-90 -180 90 180\"],\"time_start\":\"2018-07-09T00:00:00.000Z\",\"version_id\":\"002\",\"dataset_id\":\"ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"has_spatial_subsetting\":false,\"has_transforms\":false,\"has_variables\":false,\"data_center\":\"LPCLOUD\",\"short_name\":\"ECO_L2_LSTE\",\"organizations\":[\"LP DAAC\",\"NASA/JPL/ECOSTRESS\"],\"title\":\"ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\"coordinate_system\":\"CARTESIAN\",\"summary\":\"The ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) mission measures the temperature of plants to better understand how much water plants need and how they respond to stress. ECOSTRESS is attached to the International Space Station (ISS) and collects data over the conterminous United States (CONUS) as well as key biomes and agricultural zones around the world and selected FLUXNET (http://fluxnet.fluxdata.org/about/) validation sites. A map of the acquisition coverage can be found on the ECOSTRESS website (https://ecostress.jpl.nasa.gov/science).\\\\r\\\\n\\\\r\\\\nThe ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m (ECO_L2_LSTE) Version 2 data product provides atmospherically corrected land surface temperature and emissivity (LST&E) values derived from five thermal infrared (TIR) bands. The ECO_L2_LSTE data product was derived using a physics-based Temperature/Emissivity Separation (TES) algorithm. The ECO_L2_LSTE is provided as swath data and has a spatial resolution of 70 meters (m). The corresponding  ECO_L1B_GEO (https://doi.org/10.5067/ECOSTRESS/ECO_L1B_GEO.002) data product is required to georeference the ECO_L2_LSTE data product.\\\\r\\\\n\\\\r\\\\nThe ECO_L2_LSTE Version 2 data product contains layers of LST, emissivity for bands 1 through 5, quality control for LST&E, LST error, emissivity error for bands 1 through 5, wideband emissivity, Precipitable Water Vapor (PWV), cloud mask, and water mask. For acquisitions after May 15, 2019, data products contain data values for TIR bands 2, 4 and 5 only. TIR bands 1 and 3 contain fill values to accommodate direct streaming of data from the ISS as mentioned in the Known Issues section.\",\"service_features\":{\"opendap\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"esi\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false},\"harmony\":{\"has_formats\":false,\"has_variables\":false,\"has_transforms\":false,\"has_spatial_subsetting\":false,\"has_temporal_subsetting\":false}},\"orbit_parameters\":{},\"id\":\"C2076114664-LPCLOUD\",\"has_formats\":false,\"score\":1.32,\"consortiums\":[\"GEOSS\",\"EOSDIS\"],\"original_format\":\"UMM_JSON\",\"collection_data_type\":\"SCIENCE_QUALITY\",\"archive_center\":\"LP DAAC\",\"has_temporal_subsetting\":false,\"browse_flag\":false,\"platforms\":[\"ISS\"],\"online_access_flag\":true,\"links\":[{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://search.earthdata.nasa.gov/search?q=C2076114664-LPCLOUD\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://doi.org/10.5067/ECOSTRESS/ECO_L2_LSTE.002\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1495/ECOL2_User_Guide_V2.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/metadata#\",\"hreflang\":\"en-US\",\"href\":\"https://ecostress.jpl.nasa.gov/science\"},{\"length\":\"150.0MB\",\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/data#\",\"hreflang\":\"en-US\",\"href\":\"https://appeears.earthdatacloud.nasa.gov/\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf\"},{\"rel\":\"http://esipfed.org/ns/fedsearch/1.1/documentation#\",\"hreflang\":\"en-US\",\"href\":\"https://lpdaac.usgs.gov/documents/227/ECO_Earthdata_Search_Quick_Guide.pdf\"}]}]}}'\n\n\nA more convenient way to work with this information is to use json formatted data. I‚Äôm using pretty print pprint to print the data in an easy to read way.\nNote - response.json() will format our response in json - ['feed']['entry'] returns all entries that CMR returned in the request (not the same as CMR-Hits) - [0] returns the first entry. Reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\n{'archive_center': 'LP DAAC',\n 'boxes': ['-90 -180 90 180'],\n 'browse_flag': True,\n 'cloud_hosted': True,\n 'collection_data_type': 'SCIENCE_QUALITY',\n 'consortiums': ['GEOSS', 'EOSDIS'],\n 'coordinate_system': 'CARTESIAN',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n               'Instantaneous L2 Global 70 m V002',\n 'has_formats': False,\n 'has_spatial_subsetting': False,\n 'has_temporal_subsetting': False,\n 'has_transforms': False,\n 'has_variables': False,\n 'id': 'C2076090826-LPCLOUD',\n 'links': [{'href': 'https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01/ECOv002_L2T_LSTE_21247_032_19MCN_20220405T135253_0700_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'orbit_parameters': {},\n 'organizations': ['LP DAAC', 'NASA/JPL/ECOSTRESS'],\n 'original_format': 'UMM_JSON',\n 'platforms': ['ISS'],\n 'processing_level_id': '2',\n 'score': 1.32,\n 'service_features': {'esi': {'has_formats': False,\n                              'has_spatial_subsetting': False,\n                              'has_temporal_subsetting': False,\n                              'has_transforms': False,\n                              'has_variables': False},\n                      'harmony': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False},\n                      'opendap': {'has_formats': False,\n                                  'has_spatial_subsetting': False,\n                                  'has_temporal_subsetting': False,\n                                  'has_transforms': False,\n                                  'has_variables': False}},\n 'short_name': 'ECO_L2T_LSTE',\n 'summary': 'The ECOsystem Spaceborne Thermal Radiometer Experiment on Space '\n            'Station (ECOSTRESS) mission measures the temperature of plants to '\n            'better understand how much water plants need and how they respond '\n            'to stress. ECOSTRESS is attached to the International Space '\n            'Station (ISS) and collects data over the conterminous United '\n            'States (CONUS) as well as key biomes and agricultural zones '\n            'around the world and selected FLUXNET validation sites. A map of '\n            'the acquisition coverage can be found on the ECOSTRESS '\n            'website.\\r\\n'\n            'The ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n            'Instantaneous Level 2 Global 70 m (ECO_L2T_LSTE) Version 2 data '\n            'product provides atmospherically corrected land surface '\n            'temperature and emissivity (LST&E) values derived from five '\n            'thermal infrared (TIR) bands. The ECO_L2T_LSTE data product was '\n            'derived using a physics-based Temperature/Emissivity Separation '\n            '(TES) algorithm. This tiled data product is subset from the '\n            'ECO_L2G_LSTE data product using a modified version of the '\n            'Military Grid Reference System (MGRS) which divides Universal '\n            'Transverse Mercator (UTM) zones into square tiles that are 109.8 '\n            'km by 109.8 km with a 70 meter (m) spatial resolution.\\r\\n'\n            'The ECO_L2T_LSTE Version 2 data product is provided in Cloud '\n            'Optimized GeoTIFF (COG) format, and each band is distributed as a '\n            'separate COG. This product contains seven layers including LST, '\n            'LST error, wideband emissivity, quality flags, height, and cloud '\n            'and water masks. For acquisitions after May 15, 2019, data '\n            'products contain data values for TIR bands 2, 4, and 5 only. TIR '\n            'bands 1 and 3 contain fill values to accommodate direct streaming '\n            'of data from the ISS, as mentioned in the Known Issues section. '\n            'LST data generated after May 15, 2019 will only use the 3 '\n            'available bands, accuracy may be affected when compared to the '\n            'LST data that utilized all 5 bands.',\n 'time_start': '2018-07-09T00:00:00.000Z',\n 'title': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n          'Instantaneous L2 Global 70 m V002',\n 'updated': '2021-06-23T16:50:51.108Z',\n 'version_id': '002'}\n\n\nThe first response contains a lot more information than we need. We‚Äôll narrow in on a few fields to get a feel for what we have. We‚Äôll print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable.\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2T_LSTE |C2076090826-LPCLOUD\nLP DAAC | ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002 | ECO_L1B_GEO |C2076087338-LPCLOUD\nLP DAAC | ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m | ECO_L1B_RAD |C2076116385-LPCLOUD\nLP DAAC | ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2_CLOUD |C2076115306-LPCLOUD\nLP DAAC | ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2_LSTE |C2076114664-LPCLOUD\n\n\nWe know from CMR-Hits that there are 5 datasets but in some situations CMR restricts the number of results returned by each query. The default is 10 but it can be set to a maximum of 2000. If I only search for datasets that are distributed by LPCLOUD provider, we will have more number of results. We can set the page_size parameter to 50 (higher than the number of results returned) so we return all results in a single query.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            #'project': project,\n                            'page_size': 50\n                        },\n                        headers=headers\n                       )\nresponse\n\n&lt;Response [200]&gt;\n\n\n\nresponse.headers['cmr-hits']\n\n'41'\n\n\nNow, when we can re-run our for loop for the collections we now have all of the available collections listed.\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"short_name\"]} |{collection[\"id\"]}')\n\nLP DAAC | HLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30m v2.0 | HLSS30 |C2021957295-LPCLOUD\nLP DAAC | HLS Landsat Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 | HLSL30 |C2021957657-LPCLOUD\nLP DAAC | ASTER Global Digital Elevation Model V003 | ASTGTM |C1711961296-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 5-Min L2 Swath 1km V061 | MYD11_L2 |C2343114808-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MOD13Q1 |C1748066515-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V061 | MOD11A2 |C2269056084-LPCLOUD\nLP DAAC | MODIS/Terra Vegetation Indices Monthly L3 Global 1km SIN Grid V061 | MOD13A3 |C2327962326-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MOD09GA |C2202497474-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MOD11A1 |C1748058432-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices 16-Day L3 Global 250m SIN Grid V061 | MYD13Q1 |C2307290656-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity Daily L3 Global 1km SIN Grid V061 | MYD11A1 |C1748046084-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Albedo Daily L3 Global - 500m V061 | MCD43A3 |C2278860820-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance 8-Day L3 Global 500m SIN Grid V061 | MOD09A1 |C2343111356-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MOD09GQ |C2343115666-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 | MYD09GA |C2202498116-LPCLOUD\nLP DAAC | MODIS/Terra Thermal Anomalies/Fire 5-Min L2 Swath 1km V061 | MOD14 |C2271754179-LPCLOUD\nLP DAAC | MODIS/Terra Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MOD15A2H |C2218777082-LPCLOUD\nLP DAAC | MODIS/Aqua Thermal Anomalies/Fire 5-Min L2 Swath 1km V061 | MYD14 |C2278858993-LPCLOUD\nLP DAAC | MODIS/Terra Net Evapotranspiration 8-Day L4 Global 500m SIN Grid V061 | MOD16A2 |C2343113232-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance Daily L2G Global 250m SIN Grid V061 | MYD09GQ |C2343109950-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Leaf Area Index/FPAR 4-Day L4 Global 500m SIN Grid V061 | MCD15A3H |C2343110937-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua BRDF/Albedo Nadir BRDF-Adjusted Ref Daily L3 Global - 500m V061 | MCD43A4 |C2218719731-LPCLOUD\nLP DAAC | MODIS/Terra Surface Reflectance 8-Day L3 Global 250m SIN Grid V061 | MOD09Q1 |C2343112831-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 500m SIN Grid V061 | MCD12Q1 |C2484079608-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance 8-Day L3 Global 500m SIN Grid V061 | MYD09A1 |C2343113743-LPCLOUD\nLP DAAC | MODIS/Aqua Land Surface Temperature/Emissivity 8-Day L3 Global 1km SIN Grid V061 | MYD11A2 |C2269057787-LPCLOUD\nLP DAAC | ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2T_LSTE |C2076090826-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Aerosol Optical Depth Daily L2G Global 1km SIN Grid V061 | MCD19A2 |C2324689816-LPCLOUD\nLP DAAC | MODIS/Aqua Surface Reflectance 8-Day L3 Global 250m SIN Grid V061 | MYD09Q1 |C2343114343-LPCLOUD\nLP DAAC | MODIS/Aqua Vegetation Indices Monthly L3 Global 1km SIN Grid V061 | MYD13A3 |C2327957988-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 0.05Deg CMG V061 | MCD12C1 |C2484078896-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Land Cover Dynamics Yearly L3 Global 500m SIN Grid V061 | MCD12Q2 |C2484079943-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 | MCD15A2H |C2222147000-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Photosynthetically Active Radiation Daily/3-Hour L3 Global 0.05Deg CMG V061 | MCD18C2 |C2484081543-LPCLOUD\nLP DAAC | MODIS/Terra Land Surface Temperature/Emissivity 5-Min L2 Swath 1km V061 | MOD11_L2 |C2343115255-LPCLOUD\nLP DAAC | ECOSTRESS Swath Attitude and Ephemeris Instantaneous L1B Global V002 | ECO_L1B_ATT |C2076117996-LPCLOUD\nLP DAAC | ECOSTRESS Swath Geolocation Instantaneous L1B Global 70 m V002 | ECO_L1B_GEO |C2076087338-LPCLOUD\nLP DAAC | ECOSTRESS Swath Top of Atmosphere Calibrated Radiance Instantaneous L1B Global 70 m | ECO_L1B_RAD |C2076116385-LPCLOUD\nLP DAAC | ECOSTRESS Swath Cloud Mask Instantaneous L2 Global 70 m V002 | ECO_L2_CLOUD |C2076115306-LPCLOUD\nLP DAAC | ECOSTRESS Swath Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002 | ECO_L2_LSTE |C2076114664-LPCLOUD\nLP DAAC | MODIS/Terra+Aqua Downward Shortwave Radiation Daily/3-Hour L3 Global 0.05Deg CMG V061 | MCD18C1 |C2484081120-LPCLOUD"
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#searching-for-granules",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#searching-for-granules",
    "title": "Data discovery with NASA‚Äôs CMR",
    "section": "Searching for Granules",
    "text": "Searching for Granules\nIn NASA speak, Granules are files or groups of files. In this example, we will search for ECO_L2T_LSTE version 2 for a specified region of interest and datetime range.\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\nurl\n\n'https://cmr.earthdata.nasa.gov/search/granules'\n\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\ncollection_id = 'C2076090826-LPCLOUD'\ndate_range = '2022-10-20T00:00:00Z,2022-11-14T23:59:59Z'\nbbox = '-120.295181,34.210026,-119.526215,35.225021'\n\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': collection_id,\n                            'temporal': date_range,\n                            'bounding_box': bbox,\n                            #'token': token,\n                            'page_size': 200\n                            },\n                        headers=headers\n                       )\nprint(response.status_code)\n\n200\n\n\n\nprint(response.headers['CMR-Hits'])\n\n47\n\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"data_center\"]} | {granule[\"title\"]} | {granule[\"id\"]}')\n\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01 | G2530780237-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_10SGC_20221026T105945_0710_01 | G2530780962-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_001_10SGD_20221026T105945_0710_01 | G2530781111-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGD_20221026T110036_0710_01 | G2530775818-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGE_20221026T110036_0710_01 | G2530778344-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKV_20221026T110036_0710_01 | G2530780217-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKU_20221026T110036_0710_01 | G2530780251-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_11SKT_20221026T110036_0710_01 | G2530780282-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24418_002_10SGC_20221026T110036_0710_01 | G2530780296-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGE_20221030T092522_0710_01 | G2535607120-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGD_20221030T092522_0710_01 | G2535607552-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_10SGC_20221030T092522_0710_01 | G2535607555-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKT_20221030T092522_0710_01 | G2535609921-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01 | G2535610056-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24479_001_11SKV_20221030T092522_0710_01 | G2535611479-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24494_003_11SKT_20221031T083716_0710_01 | G2536450014-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGC_20221101T155724_0710_01 | G2539193798-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGD_20221101T155724_0710_01 | G2539195601-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_10SGE_20221101T155724_0710_01 | G2539196967-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_011_11SKV_20221101T155724_0710_01 | G2539199950-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKT_20221101T155816_0710_01 | G2539203544-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKV_20221101T155816_0710_01 | G2539203547-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGE_20221101T155816_0710_01 | G2539203627-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGD_20221101T155816_0710_01 | G2539205423-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_10SGC_20221101T155816_0710_01 | G2539205436-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24514_012_11SKU_20221101T155816_0710_01 | G2539206874-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGE_20221103T074954_0710_01 | G2541389219-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGD_20221103T074954_0710_01 | G2541389633-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_10SGC_20221103T074954_0710_01 | G2541390013-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKV_20221103T074954_0710_01 | G2541390707-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKT_20221103T074954_0710_01 | G2541390948-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_005_11SKU_20221103T074954_0710_01 | G2541391017-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24540_006_11SKV_20221103T075046_0710_01 | G2541390173-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_012_10SGC_20221104T151000_0710_01 | G2543733039-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_012_10SGD_20221104T151000_0710_01 | G2543733267-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_10SGD_20221104T151052_0710_01 | G2543738483-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_11SKT_20221104T151052_0710_01 | G2543739516-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24560_013_10SGC_20221104T151052_0710_01 | G2543740062-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_10SGD_20221105T142134_0710_01 | G2545346904-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_10SGE_20221105T142134_0710_01 | G2545347453-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_011_11SKV_20221105T142134_0710_01 | G2545348685-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGC_20221105T142226_0710_01 | G2545349703-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGD_20221105T142226_0710_01 | G2545349707-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_10SGE_20221105T142226_0710_01 | G2545350290-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKV_20221105T142226_0710_01 | G2545350562-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKT_20221105T142226_0710_01 | G2545350572-LPCLOUD\nLPCLOUD | ECOv002_L2T_LSTE_24575_012_11SKU_20221105T142226_0710_01 | G2545351427-LPCLOUD\n\n\n\npprint(granules[0])\n\n{'boxes': ['33.309906 -120.259598 34.3242 -119.044289'],\n 'browse_flag': True,\n 'collection_concept_id': 'C2076090826-LPCLOUD',\n 'coordinate_system': 'GEODETIC',\n 'data_center': 'LPCLOUD',\n 'dataset_id': 'ECOSTRESS Tiled Land Surface Temperature and Emissivity '\n               'Instantaneous L2 Global 70 m V002',\n 'day_night_flag': 'NIGHT',\n 'granule_size': '3.36234',\n 'id': 'G2530780237-LPCLOUD',\n 'links': [{'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.tif',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/s3#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json '\n                     '(VIEW RELATED INFORMATION)'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.json',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (VIEW RELATED INFORMATION)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml '\n                     '(EXTENDED METADATA)'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.cmr.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule (EXTENDED METADATA)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#',\n            'title': 'api endpoint to retrieve temporary credentials valid for '\n                     'same-region direct s3 access (VIEW RELATED INFORMATION)'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png'},\n           {'href': 's3://lp-prod-public/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01.png',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_water.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_cloud.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_height.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_QC.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_LST_err.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'Download '\n                     'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml'},\n           {'href': 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01/ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01_EmisWB.jpeg.aux.xml',\n            'hreflang': 'en-US',\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/browse#',\n            'title': 'This link provides direct download access via S3 to the '\n                     'granule'},\n           {'href': 'https://search.earthdata.nasa.gov/search?q=C2076090826-LPCLOUD',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n           {'href': 'https://doi.org/10.5067/ECOSTRESS/ECO_L2T_LSTE.002',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1324/ECO2_LSTE_ATBD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/380/ECO2_PSD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/299/ECO2_ASD_V1.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'},\n           {'href': 'https://lpdaac.usgs.gov/documents/1566/ECOL2-4_Grid_Tile_User_Guide_V2.pdf',\n            'hreflang': 'en-US',\n            'inherited': True,\n            'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n 'online_access_flag': True,\n 'orbit_calculated_spatial_domains': [{'start_orbit_number': '24418',\n                                       'stop_orbit_number': '24418'}],\n 'original_format': 'ECHO10',\n 'producer_granule_id': 'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01',\n 'time_end': '2022-10-26T11:00:36.970Z',\n 'time_start': '2022-10-26T10:59:45.000Z',\n 'title': 'ECOv002_L2T_LSTE_24418_001_11SKT_20221026T105945_0710_01',\n 'updated': '2022-10-28T10:41:15.849Z'}"
  },
  {
    "objectID": "how-tos/data-discovery/Data_Discovery_CMR_API.html#get-urls-to-cloud-data-assets",
    "href": "how-tos/data-discovery/Data_Discovery_CMR_API.html#get-urls-to-cloud-data-assets",
    "title": "Data discovery with NASA‚Äôs CMR",
    "section": "Get URLs to cloud data assets",
    "text": "Get URLs to cloud data assets\n\nhttps_urls = [l['href'] for l in granules[13]['links'] if 'https' in l['href'] and '.tif' in l['href']]\nhttps_urls\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_water.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_cloud.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_height.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST_err.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_EmisWB.tif']\n\n\n\ns3_urls = [l['href'] for l in granules[13]['links'] if 's3' in l['href'] and '.tif' in l['href']]\ns3_urls\n\n['s3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_water.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_cloud.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_height.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_QC.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_LST_err.tif',\n 's3://lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01/ECOv002_L2T_LSTE_24479_001_11SKU_20221030T092522_0710_01_EmisWB.tif']"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "",
    "text": "import requests\nimport netrc\nfrom datetime import datetime\nimport json\nimport os"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#read-netrc-file-to-get-edl-information",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#read-netrc-file-to-get-edl-information",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Read netrc file to get EDL information",
    "text": "Read netrc file to get EDL information\n\ndef get_edl_creds():\n    nc = netrc.netrc()\n    remoteHostName = \"urs.earthdata.nasa.gov\"\n    edl_creds = nc.authenticators(remoteHostName)\n    return {'username':edl_creds[0], 'password':edl_creds[2]}"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#specify-the-urls-for-generating-new-tokens-listing-available-tokens-and-revoking-available-tokens",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#specify-the-urls-for-generating-new-tokens-listing-available-tokens-and-revoking-available-tokens",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Specify the URLs for generating new tokens, listing available tokens, and revoking available tokens",
    "text": "Specify the URLs for generating new tokens, listing available tokens, and revoking available tokens\n\nedl_token_urls = {\n    'generate_token':'https://urs.earthdata.nasa.gov/api/users/token',\n    'list_token':'https://urs.earthdata.nasa.gov/api/users/tokens',\n    'revoke_token': 'https://urs.earthdata.nasa.gov/api/users/revoke_token'\n}"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#create-a-hidden-directory-to-store-the-output-json-file-with-edl-tokens",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#create-a-hidden-directory-to-store-the-output-json-file-with-edl-tokens",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Create a hidden directory to store the output json file with EDL tokens",
    "text": "Create a hidden directory to store the output json file with EDL tokens\n\nif not os.path.isdir('../../../.hidden_dir'):\n    os.mkdir('../../../.hidden_dir')"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#check-if-a-valid-token-exists-or-generate-a-new-token",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#check-if-a-valid-token-exists-or-generate-a-new-token",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Check if a valid token exists or generate a new token",
    "text": "Check if a valid token exists or generate a new token\n\nif len(list_tokens := requests.get(edl_token_urls['list_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password'])).json()) &lt; 1:\n    #print('No tokens available. Generating new Earthdata Login Token ...')\n    generate_token_url = \"https://urs.earthdata.nasa.gov/api/users/token\"\n    generate_token_req = requests.post(edl_token_urls['generate_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    token = generate_token_req.json()\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(token, outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\nelif datetime.strptime(list_tokens[0]['expiration_date'], \"%m/%d/%Y\") &lt; datetime.now():\n    #print('Available token is expired. Generating a new Earthdata Login Token ...')\n    revoke_token = requests.post(f\"{edl_token_urls['revoke_token']}?token={list_tokens[0]}\", auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    generate_token_req = requests.post(edl_token_urls['generate_token'], auth=(get_edl_creds()['username'], get_edl_creds()['password']))\n    token = generate_token_req.json()\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(token, outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\nelse:\n    #print('Earthdata Login Token Found ...')\n    with open(\"../../../.hidden_dir/edl_token.json\", \"w\") as outfile:\n        json.dump(list_tokens[0], outfile)\n    print(f'Your EDL token information can be found here: {os.path.abspath(\"../../../.hidden_dir/edl_token.json\")}')\n\nYour EDL token information can be found here: /home/jovyan/.hidden_dir/edl_token.json"
  },
  {
    "objectID": "how-tos/authentication/NASA_Earthdata_Login_Token.html#resources",
    "href": "how-tos/authentication/NASA_Earthdata_Login_Token.html#resources",
    "title": "Generate a NASA Earthdata Login Token",
    "section": "Resources",
    "text": "Resources\n\nhttps://wiki.earthdata.nasa.gov/display/EL/How+to+Generate+a+User+Token\nhttps://urs.earthdata.nasa.gov/documentation/for_users/user_token"
  }
]